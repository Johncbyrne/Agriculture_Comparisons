{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from cmath import sqrt\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import datetime as dt\n",
    "import sklearn\n",
    "#import folium\n",
    "import warnings\n",
    "#import pydotplus\n",
    "import six\n",
    "import sys\n",
    "#import statistics as st\n",
    "#import functions as fn\n",
    "#import cufflinks as cf\n",
    "\n",
    "#cf.set_config_file(offline=True)\n",
    "\n",
    "#prof.to_file(output_file='output.html')\n",
    "#sys.modules['sklearn.externals.six'] = six\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import ML packages\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder#for train test splitting\n",
    "from sklearn.model_selection import train_test_split#for decision tree object\n",
    "from sklearn.tree import DecisionTreeClassifier#for checking testing results\n",
    "from sklearn.metrics import classification_report, confusion_matrix#for visualizing tree \n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image \n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from explainerdashboard import RegressionExplainer, ExplainerDashboard\n",
    "\n",
    "#for interactive analysis\n",
    "from ipywidgets import interact\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRELAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRISH Crops Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "irelandagri_crops = pd.read_csv('ireland/FAOSTAT_data_5-1-2022 (2).csv')\n",
    "irelandagri_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the last 5 rows of the dataframe\n",
    "irelandagri_crops.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of variables\n",
    "irelandagri_crops.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan or missing values from multiple colu\n",
    "irelandagri_crops =  irelandagri_crops.dropna()\n",
    "irelandagri_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset information and datatypes\n",
    "irelandagri_crops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the irish crops dataset before we merge\n",
    "irelandagri_crops.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am concentrating on crops so I have removed livestock and non crops that appear under the column item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows of all rows under item that do not contain crops but contain livestock etc.\n",
    "irelandagri_crops= irelandagri_crops[irelandagri_crops[\"Item\"].str.contains(\"Asses|Beer of barley|Cattle|skimmed cow milk|Cheese, whole cow milk|Chickens|Cream fresh|Ducks|Eggs, hen, in shell|Fat, cattle|Fat, pigs|Fat, sheep|Geese and guinea fowls|Goats|Hides, cattle, fresh|Honey, natural|Hops|Horses|Lard|Margarine, short|Meat nes|Meat, cattle|Meat, chicken|Meat, duck Meat, goose and guinea fowl|Meat, horse|Meat, pig|Meat, sheep|Meat, turkey|Milk, skimmed cow|Milk, skimmed dried|Milk, whole condensed|Milk, whole dried|Milk, whole evaporated|Milk, whole fresh cow|Mules|Offals, edible, cattle|Offals, horses|Offals, pigs, edible|Offals, sheep,edible|Oil, coconut (copra)|Oil, cottonseed|Oil, groundnut|Oil, linseed|Oil, palm kernel|Oil, rapeseed|Oil, soybean|Pigs|Rapeseed|Sheep|Skins, sheep, fresh|Sugar Raw Centrifugal|Tallow|Turkeys|fresh nes|Whey, condensed|Wool, greasy|Molasses|Almonds, with shell|Buffaloes|Cottonseed|Fat, goats|Hemp tow waste|Meat, goat|Milk, whole fresh buffalo|Milk, whole fresh goat|Milk, whole fresh sheep|Offals, edible, goats|paddy (rice milled equivalent)|Skins, goat, fresh|Sorghum|Tobacco, unmanufactured|Triticale|Meat, duck|Meat, goose and guinea fowl\") == False]\n",
    "# display\n",
    "irelandagri_crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irish Climate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year: - Year\n",
    "month: - Month\n",
    "rain: - Precipitation Amount (mm)\n",
    "gdf: - Greatest daily fall (mm)\n",
    "rd: - Number of rain days (0.2mm or more)\n",
    "\"wd: - Number of wet days (1.0 mm or more) \"\n",
    "\n",
    "\n",
    "rain: - Precipitation Amount (mm)\n",
    "meant: - Mean Air Temperature (C)\n",
    "\"maxtp: - Maximum Air Temperature (C) \"\n",
    "\"mintp: - Minimum Air Temperature (C) \"\n",
    "mnmax: - Mean Maximum Temperature (C)\n",
    "mnmin: - Mean Minimum Temperature (C)\n",
    "gmin: - Grass Minimum Temperature (C)\n",
    "wdsp: - Mean Wind Speed (knot)\n",
    "mxgt: - Highest Gust (knot)\n",
    "\n",
    "https://data.gov.ie/dataset/claremorris-monthly-weather-station-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "irish_climate = pd.read_csv('ireland/irishclimate.csv')\n",
    "irish_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan or missing values from multiple columns\n",
    "irish_climate =  irish_climate.dropna()\n",
    "irish_climate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new feature called months out of month to contain the months as strings not numeric values for my visualisations later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using lambda function in order to change the months from ints 1,2 3 etc and creating a new feature months\n",
    "#This will store the months as objects/strings ie. January, February and March before Merging\n",
    "import calendar\n",
    "irish_climate['months'] = irish_climate['month'].apply(lambda x: calendar.month_name[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Assigning the columns from the irish_climate dataset to be used in a left Join on year with the crops dataset\n",
    "df_irish_climate = irish_climate[['year','months','gdf','rain','rd','wd','meant','maxtp','mintp','mnmin','gmin']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging both datasets using a left Inner Join on Year and year columnn and assiging the result to a new \n",
    "#dataframe called df_irishdata\n",
    "df_irishdata = pd.merge(irelandagri_crops,df_irish_climate,left_on=\"Year\",right_on=\"year\", how =\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspectng the new dataframe usualy the default of 5 rows is enough after the merge I like to see 20 rows to ensure\n",
    "#I have captured everything from both datasets and also to ensure I have not captured lots of duplicates\n",
    "df_irishdata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the shape of the new merged dataframe\n",
    "df_irishdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the last 5 rows of the dataframe\n",
    "df_irishdata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of variables\n",
    "df_irishdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect new dataframe for dtype info size etc.\n",
    "df_irishdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all year type to ints to ensure there are no strings for the years greater than 2009 and this covers the years for this data 2010 to 2020\n",
    "#The second 3rd line we we ensure that year is ints again sometimes after sorting or merging they can turn into strings\n",
    "\n",
    "df_irishdata = df_irishdata[df_irishdata['Year'].astype('int64') > 2009]\n",
    "df_irishdata = df_irishdata.sort_values(by=['Area', 'Year'])\n",
    "df_irishdata['Year'] = df_irishdata['Year'].astype('int64')\n",
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename our columns to be more descriptive names for some and ensure all of the uppercase ones are \n",
    "#lowercase to keep in line with naming conventions\n",
    "df_irishdata = df_irishdata.rename(columns={'Domain':'crops_domain',\n",
    "                       'Item':'crops_type',\n",
    "                       'Value':'crop_value',\n",
    "                       'Element':'crops_action',\n",
    "                       'Unit':'crop_measurement',\n",
    "                       'Area':'country',\n",
    "                       'months':'month',\n",
    "                       'gdf':'gdailyfall',\n",
    "                       'rd':'number_raindays',\n",
    "                        'wd':'number_wetdays',\n",
    "                        'meant':'mean_airtemp',\n",
    "                        'maxtp':'max_airtemp',\n",
    "                        'mintp':'min_airtemp',\n",
    "                        'gmin':'grass_minimumtemp',\n",
    "                        'mnmin':'average_temperature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect the top 5 rows to check the renaming of the columns above\n",
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder Columns/features from left to rigt\n",
    "\n",
    "irishcolumns = [\"Year\",\"country\",\"month\",\"crop_measurement\",\"crops_type\",\"crops_action\",\"crop_value\",\"rain\",\"number_raindays\",\"number_wetdays\",\"mean_airtemp\",\"max_airtemp\",\"min_airtemp\",\"mnmin\",\"grass_minimumtemp\",'average_temperature']\n",
    "df_irishdata = df_irishdata.reindex(columns = irishcolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for NAN values if any\n",
    "\n",
    "df_irishdata.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sum of null records\n",
    "\n",
    "df_irishdata.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking dataframe for missing data\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 100))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(df_irishdata.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To zoom in on the column crop-value\n",
    "missing_data = df_irishdata.iloc[:, 13:14]\n",
    "plt.figure(figsize=(20, 10))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(missing_data.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column name 'mnmin'  There are too many missing values for imputation to be used\n",
    "df_irishdata.drop(['mnmin'], axis = 1, inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "fig = px.box(df_irishdata, y='crop_value')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#\n",
    "# Box plot\n",
    "#\n",
    "sns.boxplot(df_irishdata.crop_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Distribution plot\n",
    "#\n",
    "sns.distplot(df_irishdata.crop_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable selling price is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##check rows where dependent variable is equal to zero\n",
    "df_irishdata.loc[df_irishdata['crop_value']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see for the crop value column there are over 4613 rows with the value 0 I want to find the mean value of the column\n",
    "print(df_irishdata['crop_value'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all rows with zero for the column crop_value replace them with the average value listed above 149249.4953422438\n",
    "df_irishdata['crop_value'] = df_irishdata['crop_value'].replace(0, 149249.4953422438) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 2662 rows for the column crop_value that are null I am going to replace them too with the mean average value for that feature\n",
    "# Replace NaNs in column crop_value with the\n",
    "# mean of values in the same column\n",
    "mean_value=df_irishdata['crop_value'].mean()\n",
    "df_irishdata['crop_value'].fillna(value=mean_value, inplace=True)\n",
    "print('Updated Dataframe:')\n",
    "print(df_irishdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the sum of null records again it should be 0 this time for crop_value\n",
    "\n",
    "df_irishdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation replacing the missing value with the mean value of each column/feature below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in column crop_value with the\n",
    "# mean of values in the same column\n",
    "mean_value=df_irishdata['mean_airtemp'].mean()\n",
    "df_irishdata['mean_airtemp'].fillna(value=mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value=df_irishdata['max_airtemp'].mean()\n",
    "df_irishdata['max_airtemp'].fillna(value=mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_value=df_irishdata['min_airtemp'].mean()\n",
    "df_irishdata['min_airtemp'].fillna(value=mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value=df_irishdata['average_temperature'].mean()\n",
    "df_irishdata['average_temperature'].fillna(value=mean_value, inplace=True)\n",
    "print('Updated Dataframe:')\n",
    "print(df_irishdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert grass_minumtemp which is an object to a float and then replace the missing values with the mean\n",
    "df_irishdata['grass_minimumtemp'] = pd.to_numeric(df_irishdata['grass_minimumtemp'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that grass_minimumtemp has been converted from a string to a float I have replaced the missing rows with the mean\n",
    "mean_value=df_irishdata['grass_minimumtemp'].mean()\n",
    "df_irishdata['grass_minimumtemp'].fillna(value=mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for NAN values if any\n",
    "\n",
    "df_irishdata.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics for the Target feature: crop value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the dataframe for outliers will display the min,max mean and inner and outer quariles for all features\n",
    "df_irishdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#boxplot for the target feature crop_value\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x= df_irishdata['crop_value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mean of the target feature\n",
    "mean_cropvalue= df_irishdata['crop_value'].mean()\n",
    "print(mean_cropvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the median of the target feature\n",
    "median = df_irishdata['crop_value'].median()\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mode of the target feature\n",
    "mode = df_irishdata['crop_value'].mode()\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#histplot to display the distribution of crap _value\n",
    "ax = sns.histplot(df_irishdata.crop_value)\n",
    "ax.set(xlabel='Crop Price', ylabel='Cropvalue', title ='Cropvalue Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable crop_value is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Box plot to depict Box plot Crop Value by Crop Action\n",
    "sns.boxplot(x = 'Year', y ='crop_value', data=df_irishdata, hue = 'crops_action')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title('Box plot Crop Value by Crop Action', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ireland Annual mean and average airtemp variance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata[['Year', 'average_temperature', \n",
    "                           'mean_airtemp']].pct_change().plot(kind='box', rot=90)\n",
    "plt.title(\"Ireland average temperature Variance box Plot\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ireland annual cropvalue and max airtemp variance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata[['Year', 'crop_value', \n",
    "                           'max_airtemp']].pct_change().plot(kind='box', rot=90)\n",
    "plt.title(\"Ireland Monthly Variance box Plot\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Chi-square test on a the IrishCrops dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "df_irishdata = pandas.read_csv(\"ireland/FAOSTAT_data_5-1-2022 (2).csv\")\n",
    "categorical_col = ['Domain', 'Area', 'Element', 'item', 'Unit']\n",
    "print(categorical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the crosstab() function to create a contingency table of the two selected variables to work on ‘Element’ and ‘item’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chisqt = pandas.crosstab(df_irishdata.Domain, df_irishdata.Unit, margins=True)\n",
    "print(chisqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we apply the chi2_contingency() function on the table and get the statistics, p-value and degree of freedom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency \n",
    "import numpy as np\n",
    "chisqt = pandas.crosstab(df_irishdata.Domain, df_irishdata.Unit, margins=True)\n",
    "value = np.array([chisqt.iloc[0][0:10].values,\n",
    "                  chisqt.iloc[1][0:10].values])\n",
    "print(chi2_contingency(value)[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, 1.0 is the p-value, 0.0 is the statistical value and 9 is the degree of freedom. As the p-value is greater than 0.05, we accept the NULL hypothesis and assume that the variables ‘Domain’ and ‘Unit’ are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk Test\n",
    "\n",
    "Tests whether a data sample has a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Shapiro-Wilk Normality Tes\n",
    "from scipy.stats import shapiro\n",
    "year= [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(year)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "    H0: the sample has a Gaussian distribution.\n",
    "    H1: the sample does not have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value of the Shapiro-Wilk Test is greater than 0.05, the data is normal. If it is below 0.05, the data significantly deviate from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Shapiro-Wilk Normality Test\n",
    "from scipy.stats import shapiro\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the result is >0.05 the result is normal, the data significantly deviate from a normal distribution if <0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro Test on Crops Action variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import useful library\n",
    "import numpy as np\n",
    "from numpy.random import poisson\n",
    "from numpy.random import seed\n",
    "from scipy.stats import shapiro\n",
    "from numpy.random import randn\n",
    "  \n",
    "seed(0)\n",
    "# Create data\n",
    "crops_action = poisson(5, 200)\n",
    "  \n",
    "# conduct the  Shapiro-Wilk Test\n",
    "shapiro(crops_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the above example, the p-value is 0.0001 which is less than the alpha(0.5) then we reject the null hypothesis i.e. we have sufficient evidence to say that sample does not come from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson’s Correlation Coefficient\n",
    "\n",
    "Tests whether two samples have a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_action= [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = pearsonr(crop_value, crop_action)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman’s Rank Correlation\n",
    "\n",
    "Tests whether two samples have a monotonic relationship.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample can be ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_measurement = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = spearmanr(crop_value, crop_measurement)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D’Agostino’s K^2 Test - Normality Test\n",
    "\n",
    "Tests whether a data sample has a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "df_irishdata = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = normaltest(df_irishdata)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "    H0: the sample has a Gaussian distribution.\n",
    "    H1: the sample does not have a Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Squared Test\n",
    "\n",
    "Tests whether two categorical variables are related or independent.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations used in the calculation of the contingency table are independent.\n",
    "    25 or more examples in each cell of the contingency table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Chi-Squared Test\n",
    "from scipy.stats import chi2_contingency\n",
    "crop_value = [[10, 20, 30],[6,  9,  17]]\n",
    "stat, p, dof, expected = chi2_contingency(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE can see that the P - value for crop value is greater than 0.5 and is probably independant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student’s t-test\n",
    "\n",
    "Tests whether the means of two independent samples are significantly different.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Student's t-test\n",
    "from scipy.stats import ttest_ind\n",
    "max_airtemp = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "min_airtemp = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(max_airtemp, min_airtemp)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the means of the samples are equal.\n",
    "    H1: the means of the samples are unequal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Statistical Hypothesis Tests\n",
    "\n",
    "This section lists statistical tests that you can use to compare data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired Student’s t-test\n",
    "\n",
    "Tests whether the means of two paired samples are significantly different.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n",
    "    Observations across each sample are paired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Student's t-test\n",
    "from scipy.stats import ttest_ind\n",
    "rain = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "year = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(rain, year)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpretation\n",
    "\n",
    "    H0: the means of the samples are equal.\n",
    "    H1: the means of the samples are unequal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE can see that the P value for Year is greater than 0.5 and is therefore of the same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that the P Value for all test s above has not been greater than 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Variance Test (ANOVA)\n",
    "\n",
    "Tests whether the means of two or more independent samples are significantly different.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Analysis of Variance Test\n",
    "from scipy.stats import f_oneway\n",
    "grass_minimumtemp = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_value = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "crops_action = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\n",
    "stat, p = f_oneway(grass_minimumtemp, crop_value, crops_action)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the means of the samples are equal.\n",
    "    H1: one or more of the means of the samples are unequal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric Statistical Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ests whether the distributions of two independent samples are equal or not.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample can be ranked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Mann-Whitney U Test\n",
    "from scipy.stats import mannwhitneyu\n",
    "crop_measurement = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_type = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = mannwhitneyu(crop_measurement, crop_type)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the distributions of both samples are equal.\n",
    "    H1: the distributions of both samples are not equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wilcoxon Signed-Rank Test\n",
    "\n",
    "Tests whether the distributions of two paired samples are equal or not.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample can be ranked.\n",
    "    Observations across each sample are paired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of the Wilcoxon Signed-Rank Test\n",
    "from scipy.stats import wilcoxon\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crops_action = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = wilcoxon(crop_value, crops_action)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the distributions of both samples are equal.\n",
    "    H1: the distributions of both samples are not equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape and size of the dataframe\n",
    "print(\"Shape of the dataset is : \",df_irishdata.shape)\n",
    "print(\"Size of the dataset is : \",df_irishdata.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the datatypes of all columns we can see there are alot of objects later I will change these to onehotencoding for the linear regression\n",
    "df_irishdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Distribution of the Target variable/feature crop_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of crop_value target variable seaborn distplot\n",
    "sns.distplot(df_irishdata['crop_value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop_value is not normally distributed the distribution is skewed left because it looks pulled out to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.regplot(x = df_irishdata[\"Year\"], y = df_irishdata[\"crop_value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.regplot(x = df_irishdata[\"Year\"], y = df_irishdata[\"number_wetdays\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the pearson correlation coefficients. This helps us understand the extent to which two variables are correlated. We will be able to see both the strength of the correlation as well as the direction and use that to make a decision on the exclusion of predictive variables that display multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create correlation matrix displaying pearson correlation coefficients for all variables\n",
    "corr_matrix = df_irishdata.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visually examine the relationships between the measurable variables via a scatterplot using a randomly selected sample size of 1,000. We also see the histograms for each of the measurable variables displayed across the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pair grid with histograms and scatterplots using seaborn pairgrid\n",
    "irish_data_sample = df_irishdata.sample(1000)\n",
    "p = sns.PairGrid(data=irish_data_sample, vars=[\"Year\",\"country\",\"month\",\"crop_measurement\",\"crops_type\",\"crops_action\",\"crop_value\",\"rain\",\"number_wetdays\",\"number_raindays\",\"grass_minimumtemp\"])\n",
    "p.map_diag(plt.hist)\n",
    "p.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a roughly normal distribution for available_bike_stands.  A decision will need to be made as to which should be used when we create the linear regression model. crops_type will be the Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "Year = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(Year)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "month = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(month)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create point plot using seaborn pointplot to display Crop Value by month for each year from 2010 to 2020\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_irishdata, x='Year', y='crop_value', ci=\"sd\", hue='month', palette='gist_rainbow_r')\n",
    "ax1.set(title='Crop value by month for each year from 2010 to 2020')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see a sharp increase from 2017 and again in 2020 and overall 2020 was the highest year for crops value.  The best month was July in 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point plot comparing Crop Value and the type of crop action for each year from 2010 to 2020 using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_irishdata, x='Year', y='crop_value', ci=\"sd\", hue='crops_action', palette='YlGnBu')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax1.set(title='Crop Value and the type of crop action for each year from 2010 to 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the seaborn subplot that it is 2020 was the year the crop value was the highest but really it did not change that much year on year over the decade. It is depicted though that stocks increased significantly from 2017 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point plot Count of the type of crops and value for each year from 2010 to 2020 using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_irishdata, x='Year', y='crop_value', ci=\"sd\", hue='crops_type', palette='prism_r')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax1.set(title='Count of the type of crops and value for each year from 2010 to 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barley was the most sold over the decade in 2018 the highest amount crop value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create point plot comparing Rainfall by month over the decade using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_irishdata, x='Year', y='rain', ci=\"sd\", hue='month', palette='gist_rainbow_r')\n",
    "ax1.set(title='Rainfall by month over the decade')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see over the decade we had the highest rainfall in January 2020 and the lowest in October 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop value by crop action for each year over the decade using seaborn pointplot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.pointplot(data=df_irishdata, x='Year', y='crop_value', hue='crops_action', ax=ax)\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax.set(title='Crop value by crop action for each year over the decade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest crop yield was in 2018 the lowest was in 2013.  2012 was the highest production of crops and 2018 was slighty the highest year for crop area harvested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greatest rd = rainfall by month throughout the decade using seaborn pointplot\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.pointplot(data=df_irishdata, x='Year', y='grass_minimumtemp', hue='month', ax=ax)\n",
    "ax.set(title='Grass minimum temperature by month throughout the decade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "June 2013 was the year and month with the highest grass temperature.  April 2010 was the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the crop price from 2010 to 2020\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='Year', y='crop_value',data=df_irishdata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the rain for each year\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='Year', y='rain',data=df_irishdata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly express display a barplot for the crop value for each year\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_irishdata, x='Year', y='crop_value',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotly bar plot to display number of wet days\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_irishdata, x='Year', y='number_wetdays')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create box plots for time related variables using seaborn boxplot\n",
    "figure, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4)\n",
    "figure.set_size_inches(24, 8)\n",
    "\n",
    "sns.boxplot(data=df_irishdata, x='Year', y='crop_value', ax=ax1)\n",
    "sns.boxplot(data=df_irishdata, x='month', y='crop_value', ax=ax2)\n",
    "sns.boxplot(data=df_irishdata, x='Year', y='rain', ax=ax3)\n",
    "sns.boxplot(data=df_irishdata, x='month', y='average_temperature', ax=ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot to display count of crop value over each year using seaborn barplot\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.barplot(data=df_irishdata, x='Year', y='crop_value', ax=ax)\n",
    "ax.set(title='Count of crop value for each year over the decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Barplot to display count of average temp each year over the decade using seaborn barplot\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.barplot(data=df_irishdata, x='Year', y='average_temperature', ax=ax)\n",
    "ax.set(title='Count of average temperature for each year over the decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn regplot used to display the relationship between Relation between year and crop value\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(20,6))\n",
    "sns.regplot(x=df_irishdata['Year'], y=df_irishdata['crop_value'], ax=ax1)\n",
    "ax1.set(title=\"Relation between year and crop value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn distplot used to depict the distribution of cropvalue and the Theoritical quantiles of that feature\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,6))\n",
    "sns.distplot(df_irishdata['crop_value'], ax=ax1)\n",
    "ax1.set(title='Distribution of the crop_value')\n",
    "qqplot(df_irishdata['crop_value'], ax=ax2, line='s')\n",
    "ax2.set(title='Theoritical quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seaborn distplot used to depict the distribution of max_airtemp and the Theoritical quantiles of grass_minimumtemp\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,6))\n",
    "sns.distplot(df_irishdata['max_airtemp'], ax=ax1)\n",
    "ax1.set(title='Distribution of the consumer_value')\n",
    "qqplot(df_irishdata['grass_minimumtemp'], ax=ax2, line='s')\n",
    "ax2.set(title='Theoritical quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Coorelation Matrix to compare the relationships and correlation between features\n",
    "corr = df_irishdata.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, annot_kws={'size':15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1 indicates a perfectly negative linear correlation between two variables.\n",
    "0 indicates no linear correlation between two variables.\n",
    "1 indicates a perfectly positive linear correlation between two variables\n",
    "\n",
    "We can see that Year and Average Temperature have a Perfectly Positive Linear Correlation between both features.\n",
    "0.75 Rain and Number of Wetdays are correlated. \n",
    "0.64 Max airtemp and grass minimum temp are correlated.\n",
    "0.79 grass_minimumtemp and mean/average airtemp are correlated.\n",
    "0.86 grass_minimumtemp and minimum airtemp are correlated.\n",
    "\n",
    "The target Crop_value we can see already does not have any correlation with any other feature :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a seaborn relplot to plot 3 data points to see the relationship between 2 variables using my own color pallete hue is hour time of day\n",
    "sns.relplot(x=\"Year\", y=\"crop_value\", hue=\"month\", palette=\"ch:r=-.5,l=.75\", data=df_irishdata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two arrays to compare coefficients for the 2 highest correlated features\n",
    "cropgrasstemp=np.array(df_irishdata[\"grass_minimumtemp\"])\n",
    "cropairtemp=np.array(df_irishdata[\"min_airtemp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#how to work out coefficients between gdp and \n",
    "np.corrcoef (cropgrasstemp, cropairtemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can see how highly correlated both features are now lets plot them below in an interactive visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result tells us that there is a high correlation between grass_minimumtemp and min_airtemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualisation using Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Were creating two variables for our min and max values for the slider\n",
    "xmin,xmax=min(df_irishdata[\"Year\"]), max(df_irishdata[\"Year\"])\n",
    "ymin,ymax=min(df_irishdata[\"crop_value\"]), max(df_irishdata[\"crop_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using year as the animation frame, and as we go through each year the values will change want to group it by month\n",
    "#The color will be the crops type each crop will have a different color, using hover for the year to give us info\n",
    "#Split up the sections by usi by crops Action (yield, Harvest and Productions) \n",
    "\n",
    "fig= px.scatter(df_irishdata, x=\"Year\", y=\"crop_value\", animation_frame =\"Year\",\n",
    "               animation_group=\"month\",color=\"crops_type\", hover_name=\"month\", \n",
    "               facet_col=\"crops_action\", width=1580, height=400, log_x=True, size_max=10000,\n",
    "               range_x=[xmin,xmax],range_y=[ymin,ymax])\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using plotly to display barchart Value of crops produced by Ireland each year crops type action and month displayed\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_irishdata, x='Year', y='crop_value',\n",
    "                hover_data=['crops_type','month'], color='crops_action',\n",
    "                labels={'country':'Value of crops produced by Ireland each year'}, height=400, title='Value of crops produced by Ireland each year')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking the crops present\n",
    "df_irishdata['crops_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mean of all of our Climate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking average climatic and soil requirements\n",
    "\n",
    "print(\" Average Ratio of grass_temp in the Soil : {0:.2f}\".format(df_irishdata['grass_minimumtemp'].mean()))\n",
    "print(\" Average Ratio of Rain in the Soil : {0:.2f}\".format(df_irishdata['rain'].mean()))\n",
    "print(\" Average Ratio of average_temperature in the Soil : {0:.2f}\".format(df_irishdata['average_temperature'].mean()))\n",
    "print(\" Average max_airtemp in Celsius : {0:.2f}\".format(df_irishdata['max_airtemp'].mean()))\n",
    "print(\" Average Relative min_airtemp in % : {0:.2f}\".format(df_irishdata['min_airtemp'].mean()))\n",
    "print(\" Average number_wetdays : {0:.2f}\".format(df_irishdata['number_wetdays'].mean()))\n",
    "print(\" Average number_raindays : {0:.2f}\".format(df_irishdata['number_raindays'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the detailed descriptive statistics for each crop \n",
    "#Creating a list of all  crops and then can use descriptive stats to get the mean, meadian and mode \n",
    "#(min, mean and max of each climate factors for each of the crops using interact.\n",
    "                                                                                                      \n",
    "@interact\n",
    "def summary(crops = list(df_irishdata['crops_type'].value_counts().index)):\n",
    "    x = df_irishdata[df_irishdata['crops_type'] == crops]\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Grass Temp\")\n",
    "    print(\"Minimum Grass_temp required:\", x['grass_minimumtemp'].min())\n",
    "    print(\"Average Grass_temp required:\", x['grass_minimumtemp'].mean())\n",
    "    print(\"Maximum Grass_temp required:\", x['grass_minimumtemp'].max())\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Rain\")\n",
    "    print(\"Minimum Rain required:\", x['rain'].min())\n",
    "    print(\"Average Rain required:\", x['rain'].mean())\n",
    "    print(\"Maximum Rain required:\", x['rain'].max())\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Average Temperature\")\n",
    "    print(\"Minimum average_temperature required:\", x['average_temperature'].min())\n",
    "    print(\"Average average_temperature:\", x['average_temperature'].mean())\n",
    "    print(\"Maximum average_temperature:\", x['average_temperature'].max())\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Air Temperature\")\n",
    "    print(\"Minimum Temperature required: {0:.2f}\".format(x['max_airtemp'].min()))\n",
    "    print(\"Average Temperature required: {0:.2f}\".format(x['max_airtemp'].mean()))\n",
    "    print(\"Maximum Temperature required: {0:.2f}\".format(x['max_airtemp'].max()))\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Minimum AirTemp\")\n",
    "    print(\"Minimum min_airtemp required: {0:.2f}\".format(x['min_airtemp'].min()))\n",
    "    print(\"Average min_airtemp required: {0:.2f}\".format(x['min_airtemp'].mean()))\n",
    "    print(\"Maximum min_airtemprequired: {0:.2f}\".format(x['min_airtemp'].max()))\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Number of Wetdays\")\n",
    "    print(\"Minimum number_wetdays required: {0:.2f}\".format(x['number_wetdays'].min()))\n",
    "    print(\"Average number_wetdaysrequired: {0:.2f}\".format(x['number_wetdays'].mean()))\n",
    "    print(\"Maximum number_wetdays required: {0:.2f}\".format(x['number_wetdays'].max()))\n",
    "    print(\"...........................................\")\n",
    "    print(\"Statistics for Number of Rainy Days\")\n",
    "    print(\"Minimum number_raindays required: {0:.2f}\".format(x['number_raindays'].min()))\n",
    "    print(\"Average number_raindays required: {0:.2f}\".format(x['number_raindays'].mean()))\n",
    "    print(\"Maximum number_raindays required: {0:.2f}\".format(x['number_raindays'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing and getting the perfect climate conditions for each individual crop by using the dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Comparing Average requirement and conditions for each crop\n",
    "\n",
    "@interact\n",
    "def compare(climateconditions = ['grass_minimumtemp', 'rain', 'average_temperature', 'max_airtemp', 'min_airtemp', 'number_wetdays', 'number_raindays']):\n",
    "    print(\"Average Value for\", climateconditions, \"is {0:.2f}\".format(df_irishdata[climateconditions].mean()))\n",
    "    print(\"...........................................\")\n",
    "    print(\"Apples : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Apples')][climateconditions].mean()))\n",
    "    print(\"Apricots : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Apricots')][climateconditions].mean()))\n",
    "    print(\"Avocados : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Avocados')][climateconditions].mean()))\n",
    "    print(\"Bananas : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Bananas')][climateconditions].mean()))\n",
    "    print(\"Barley : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Barley')][climateconditions].mean()))\n",
    "    print(\"Beans, green : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Beans, green')][climateconditions].mean()))\n",
    "    print(\"Berries nes : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Berries nes')][climateconditions].mean()))\n",
    "    print(\"Broad beans, horse beans, dry : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Broad beans, horse beans, dry')][climateconditions].mean()))\n",
    "    print(\"Cabbages and other brassicas : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Cabbages and other brassicas')][climateconditions].mean()))\n",
    "    print(\"Carrots and turnips : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Carrots and turnips')][climateconditions].mean()))\n",
    "    print(\"Cauliflowers and broccoli : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Cauliflowers and broccoli')][climateconditions].mean()))\n",
    "    print(\"Cereals nes : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Cereals nes')][climateconditions].mean()))\n",
    "    print(\"Chestnut : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Chestnut')][climateconditions].mean()))\n",
    "    print(\"Chillies and peppers, green : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Chillies and peppers, green')][climateconditions].mean()))\n",
    "    print(\"Cucumbers and gherkins : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Cucumbers and gherkins')][climateconditions].mean()))\n",
    "    print(\"Currants : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Currants')][climateconditions].mean()))\n",
    "    print(\"Figs : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Figs')][climateconditions].mean()))\n",
    "    print(\"Lettuce and chicory : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Lettuce and chicory')][climateconditions].mean()))\n",
    "    print(\"Linseed : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Linseed')][climateconditions].mean()))\n",
    "    print(\"Mushrooms and truffles : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Mushrooms and truffles')][climateconditions].mean()))\n",
    "    print(\"Wheat : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Wheat')][climateconditions].mean()))\n",
    "    print(\"Vegetables, leguminous nes : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Vegetables, leguminous nes')][climateconditions].mean()))\n",
    "    print(\"Tomatoes : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Tomatoes')][climateconditions].mean()))\n",
    "    print(\"Sugar beet : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Sugar beet')][climateconditions].mean()))\n",
    "    print(\"Strawberries : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Strawberries')][climateconditions].mean()))\n",
    "    print(\"Spinach : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Spinach')][climateconditions].mean()))\n",
    "    print(\"Rye : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Rye')][climateconditions].mean()))\n",
    "    print(\"Raspberries : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Raspberries')][climateconditions].mean()))\n",
    "    \n",
    "    print(\"Pumpkins, squash and gourds : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Pumpkins, squash and gourds')][climateconditions].mean()))\n",
    "    print(\"Potatoes : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Potatoes')][climateconditions].mean()))\n",
    "    print(\"Peas, green : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Peas, green')][climateconditions].mean()))\n",
    "    print(\"Peas, dry : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Peas, dry')][climateconditions].mean()))\n",
    "    print(\"Onions, dry : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Onions, dry')][climateconditions].mean()))\n",
    "    print(\"Oats : {0:.2f}\".format(df_irishdata[(df_irishdata['crops_type'] == 'Oats')][climateconditions].mean()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the below and above Average Conditions for each crop\n",
    "\n",
    "@interact\n",
    "def compare(climateconditions = ['grass_minimumtemp', 'rain', 'average_temperature', 'max_airtemp', 'min_airtemp', 'number_wetdays', 'number_raindays']):\n",
    "    print(\"Crops that require greater than average\", climateconditions, '\\n')\n",
    "    print(df_irishdata[df_irishdata[climateconditions] > df_irishdata[climateconditions].mean()]['crops_type'].unique())\n",
    "    print(\"...........................................\")\n",
    "    print(\"Crops that require less than average\", climateconditions, '\\n')\n",
    "    print(df_irishdata[df_irishdata[climateconditions] <= df_irishdata[climateconditions].mean()]['crops_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the distributiion for each climate weather condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the distributiion for each climate weather condition\n",
    "\n",
    "plt.subplot(3,4,1)\n",
    "sns.histplot(df_irishdata['grass_minimumtemp'], color=\"green\")\n",
    "plt.xlabel('Grass', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3,4,2)\n",
    "sns.histplot(df_irishdata['rain'], color=\"blue\")\n",
    "plt.xlabel('Rain', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3,4,3)\n",
    "sns.histplot(df_irishdata['average_temperature'], color=\"orange\")\n",
    "plt.xlabel('Average Teperature', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,4,4)\n",
    "sns.histplot(df_irishdata['max_airtemp'], color=\"red\")\n",
    "plt.xlabel('Max Air Temperature', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,4,5)\n",
    "sns.histplot(df_irishdata['min_airtemp'], color=\"lightblue\")\n",
    "plt.xlabel('Minumum Air Temp', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,4,6)\n",
    "sns.histplot(df_irishdata['number_wetdays'], color=\"darkblue\")\n",
    "plt.xlabel('number_wetdays', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2,4,7)\n",
    "sns.histplot(df_irishdata['number_raindays'], color=\"cyan\")\n",
    "plt.xlabel('number_raindays', fontsize = 12)\n",
    "plt.grid()\n",
    "\n",
    "plt.suptitle('Distribution for Agricultural Conditions', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entering in conditional and bitwise operations to find out crops with unusual requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that crops those have unusual requirements\n",
    "\n",
    "print(\"Some Interesting Patterns\")\n",
    "print(\"...........................................\")\n",
    "print(\"Crops that require very High Ratio of Grass Minuimum Temp of Soil:\", df_irishdata[df_irishdata['grass_minimumtemp'] > 7]['crops_type'].unique())\n",
    "print(\"Crops that require very High Ratio of Rainfall:\", df_irishdata[df_irishdata['rain'] > 254]['crops_type'].unique())\n",
    "print(\"Crops that require very High Ratio of Average Temperature:\", df_irishdata[df_irishdata['average_temperature'] > 12]['crops_type'].unique())\n",
    "print(\"Crops that require very High Air Temperature:\", df_irishdata[df_irishdata['max_airtemp'] > 30]['crops_type'].unique())\n",
    "print(\"Crops that require very Low Air Temperature:\", df_irishdata[df_irishdata['min_airtemp'] < 10]['crops_type'].unique())\n",
    "print(\"Crops that require number_wetdays:\", df_irishdata[df_irishdata['number_wetdays'] > 10]['crops_type'].unique())\n",
    "print(\"Crops that require number_raindays:\", df_irishdata[df_irishdata['number_raindays'] < 10]['crops_type'].unique())\n",
    "print(\"Crops that require very Low Air Temp:\", df_irishdata[df_irishdata['min_airtemp'] <5]['crops_type'].unique())\n",
    "print(\"Crops that require very High Air:\", df_irishdata[df_irishdata['max_airtemp'] >30]['crops_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using conditional statements and breaking the airtemp up into 3 seasons summer, winter and monsoon rain and listing\n",
    "the crops that fall under each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which crop to be grown according to the season\n",
    "\n",
    "print(\"Summer Crops\")\n",
    "print(df_irishdata[(df_irishdata['max_airtemp'] >28) & (df_irishdata['grass_minimumtemp'] >3)]['crops_type'].unique())\n",
    "print(\"...........................................\")\n",
    "print(\"Winter Crops\")\n",
    "print(df_irishdata[(df_irishdata['max_airtemp'] < 11) & (df_irishdata['min_airtemp'] <-5)]['crops_type'].unique())\n",
    "print(\"...........................................\")\n",
    "print(\"Monsoon Crops\")\n",
    "print(df_irishdata[(df_irishdata['number_raindays'] > 30) & (df_irishdata['number_raindays'] < 300)]['crops_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the categorical features for the kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#removing the labels column\n",
    "x = df_irishdata.drop(['crops_type','crops_action','crop_measurement','month','country'], axis=1)\n",
    "\n",
    "#selecting all the values of data\n",
    "x = x.values\n",
    "\n",
    "#checking the shap\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. As you know, if k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining the optimum number of clusters within the Dataset\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 2000, n_init = 10, random_state = 0)\n",
    "    km.fit(x)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "#Plotting the results\n",
    "\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow Method', fontsize = 20)\n",
    "plt.xlabel('No of Clusters')\n",
    "plt.ylabel('wcss')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of k clusters to be used is 2 as used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of K Means algorithm to perform Clustering analysis\n",
    "\n",
    "km = KMeans(n_clusters = 2, init = 'k-means++',  max_iter = 2000, n_init = 10, random_state = 0)\n",
    "y_means = km.fit_predict(x)\n",
    "\n",
    "#Finding the results\n",
    "a = df_irishdata['crops_type']\n",
    "y_means = pd.DataFrame(y_means)\n",
    "z = pd.concat([y_means, a], axis = 1)\n",
    "z = z.rename(columns = {0: 'cluster'})\n",
    "\n",
    "#Checking the clusters for each crop\n",
    "print(\"Lets Check the results after applying K Means Clustering Analysis \\n\")\n",
    "print(\"Crops in First Cluster:\", z[z['cluster'] == 0]['crops_type'].unique())\n",
    "print(\"...........................................\")\n",
    "print(\"Crops in Second Cluster:\", z[z['cluster'] == 1]['crops_type'].unique())\n",
    "print(\"...........................................\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Dataset for predictive modelling\n",
    "\n",
    "y = df_irishdata['crops_type']\n",
    "x = df_irishdata.drop(['crops_type','country','month','crop_measurement','crops_type','crops_action'], axis=1)\n",
    "\n",
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training and testing sets for results validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"The Shape Of x train:\", x_train.shape)\n",
    "print(\"The Shape Of x test:\", x_test.shape)\n",
    "print(\"The Shape Of y train:\", y_train.shape)\n",
    "print(\"The Shape Of y test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Predictive Model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model performance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Printing the Confusing Matrix\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot = True, cmap = 'Wistia')\n",
    "plt.title('Confusion Matrix For Logistic Regression', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Printing the Classification Report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head of dataset\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have entered in the values below for the suggested prediction though the accuracy is very low for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict((np.array([[2010,620.0,79.9,14, 12, 5.7, 13.6, -6.2, -10.1, 1.2]])))\n",
    "print(\"The Suggested Crop for given climatic condition is :\",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Irish - Models using crop value as the target dependant variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive stats on the target Crop Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking the crop_value present\n",
    "df_irishdata['crop_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the dataframe to retrive the mean,min and max values for each column the quarters and std deviation\n",
    "df_irishdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the mean of the target / dependant variable\n",
    "mean_cropval= df_irishdata['crop_value'].mean()\n",
    "print(mean_cropval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the median of the target / dependant variable\n",
    "median_cropval= df_irishdata['crop_value'].median()\n",
    "print(median_cropval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the mode of the target / dependant variable\n",
    "mode_cropval= df_irishdata['crop_value'].mode()\n",
    "print(mode_cropval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Histplot of the crops value yield\n",
    "ax = sns.histplot(df_irishdata.crop_value)\n",
    "ax.set(xlabel='yield', ylabel='cropsvalue', title ='Crop Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable selling price is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.histplot(df_irishdata.crop_value, kde=True, color ='green')\n",
    "ax.set(xlabel='produced', ylabel='Value of Crops', title ='Crops Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable selling price is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot of crops by year\n",
    "sns.boxplot(x = 'Year', y ='crop_value', data=df_irishdata, hue = 'Year')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title('Box plot crop_value', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by year and aggreating the sum total\n",
    "cropgrouping = df_irishdata.groupby('Year', axis = 0).sum()\n",
    "cropgrouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie chart to display the average temperature each year in ireland\n",
    "cropgrouping['average_temperature'].plot(kind='bar')\n",
    "\n",
    "plt.title('Average Temperature for each year')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normality Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section statistics will be done on all the data for each country. Additionally the datasets will be checked for normal distributions and compared with the proper statistical test. This will allow for acceptance or rejection of null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the Csv after been explored and cleaned has now been saved so as I can just read this in for my models going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the data has been explored and cleaned saving it to be used for the models\n",
    "df_irishdata.to_csv('df_irishmodels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in df_irishmodels.csv into the df_irishdata frame for linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage to this is I dont have to run the code from the top of the page to the bottom every time, I just need to run the imports beforhand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')\n",
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_irishdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking Correlation using heatmap\n",
    "sns.heatmap(df_irishdata.corr(),annot=True,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "-1 indicates a perfectly negative linear correlation between two variables.\n",
    "0 indicates no linear correlation between two variables.\n",
    "1 indicates a perfectly positive linear correlation between two variables\n",
    "\n",
    "We can see that Year and Average Temperature have a Perfectly Positive Linear Correlation between both features.\n",
    "0.85 Rain and Number of Wetdays are correlated. \n",
    "0.64 Max airtemp and grass minimum temp are correlated.\n",
    "0.79 grass_minimumtemp and mean/average airtemp are correlated.\n",
    "0.86 grass_minimumtemp and minimum airtemp are correlated.\n",
    "\n",
    "The target Crop_value we can see already does not have any correlation with any other feature :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.catplot(x='crops_action', data=df_irishdata, palette=\"hls\",kind='count',hue='Year')\n",
    "plt.xlabel(\"Crop_type\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "plt.title(\"Crops_action Grouped Count by yield, harvest and production\", fontsize=20)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Crop action harvest adn production were both highest in 2020\n",
    "2. 2013 was the lowest year for all 3 type s harvest, yield and Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Year\" ,y=\"crop_value\",hue=\"crops_action\",data=df_irishdata)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "    Clearly observed that the crop yield for years 2018, 2019 and 2020 were best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use the describe function \n",
    "df_irishdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. I dont see any obvious outliers in the data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_irishdata.drop(columns=[\"Unnamed: 0\",\"country\",\"month\",\"crop_measurement\"],axis=1,inplace=True)\n",
    "df_irishdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several categorical variables and will need to transform them into dummy variables with binary values in order to incorporate them into our model. This is done because despite having int values, they are not ordinal variables. In order to avoid multicollinearity we will also have to drop one of the dummy variables from each set. For example, 'month','crops_action','crops_type','country' the first variable will be dropped for our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_irishdata = pd.get_dummies(df_irishdata, columns = ['crops_action','crops_type'],drop_first = True)\n",
    "\n",
    "\n",
    "# inspect bike_data df with added dummy variables\n",
    "df_irishdata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dummy variables we want to inspect the new full list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for Nans\n",
    "df_irishdata.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Multiple Regression and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start we will create a multiple regression using all of the independant features to get a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using backward selection where we start with all of the predictor variables in the model, and after evaluating the variables, remove less useful predictors one at a time or in batches for categorical variables. We will take a look at the strength of the model (adj r2) and the statistical significance (P-value) of the indepenent variables and determine how to proceed in revising the model.\n",
    "\n",
    "Below I will be creating a multiple regression model which is a supervised machine learning parametric method. To do that I will split our data into a training set (70%) and a test set(30%) standard. I will be fiting the multiple linear regression to the features and data from the training set and then testing the performance of the model against the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_irishdata['crop_value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_irishdata.drop('crop_value', axis=1)\n",
    "\n",
    "# split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)# 70% training and 30% test\n",
    "X_train = sm.add_constant(X_train)\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Linear Regression using all independant variables/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data to linear regression\n",
    "mlr1 = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# view OLS regression results\n",
    "print(mlr1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R Squared, also known as the coefficient of determination, displays the variation in the depdendent target variable y as explained by the independent predictor x variables. In other words, the percentage of the prediction outcome that can be attributed to the predictor variables of the model. Another way to look at it is R2 = explained variation / total variation. The higher the number the better the data fits the model in question.\n",
    "\n",
    "Adjusted R Squared is used when multiple independent predictor variables exist and includes a penalty for adding additional predictors. This allows us to compare the effectiveness of different models with differing numbers of predictor variables. This is required because as I add predictors, R2 will always continue to increase, even if there is just a chance correlation between variables. In general a parsimonious model is preferred given that it meets reasonable Adjusted R Squared and statistical significance criteria.\n",
    "\n",
    "P-Values measure the statistical significance of each variable in the model within the context of all variables in the model. It is essentially a measure of the liklihood of achieving results as extreme as were observed given a null hypothesis. In other words, the likeliness of the results being explained by random chance. A very low P-value is desired with .05 or .01 often being used as the standard depending on the context and several factors.\n",
    "\n",
    "Model 1\n",
    "\n",
    "Here we take a look at the strength of the model (adj r2) and the statistical significance (P-value) of the indepenent variables.\n",
    "\n",
    "Adj R2:       0.363\n",
    "\n",
    "There are many variables that have an unacceptably high P-value, particularly several of the categorical ones. Given that they are binary categorical dummy variables, it doesn't make much sense to keep some of them and drop only the problematic ones. For our second model we will be dropping all of variables below.\n",
    "\n",
    "max_airtemp                                             \n",
    "min_airtemp                                               \n",
    "grass_minimumtemp\n",
    "\n",
    "rain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Linear Regression (Dropping all of the month columns the P values are high) backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the features below with a high p-value from the training model\n",
    "X_train2 = X_train.drop(['rain','grass_minimumtemp','min_airtemp','max_airtemp'], axis=1)\n",
    "mlr2 = sm.OLS(y_train, X_train2).fit()\n",
    "print(mlr2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still several variables that have an unacceptably high P-value, particularly the mean_airtemp                      average_temperature  variables. We will be dropping all of the hour variables as well for our third model and observing the change.\n",
    "\n",
    "There is no change with the Adj R2: 0.363 value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Linear Regression (Dropping all of the hour columns) backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the Temperature features with a high p-value from the training model\n",
    "X_train3 = X_train2.drop(['mean_airtemp','average_temperature'], axis=1)\n",
    "mlr3 = sm.OLS(y_train, X_train3).fit()\n",
    "print(mlr3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3\n",
    "\n",
    "For our third model, we see that the adjusted R2 has stayed the same after dropping all hour features.\n",
    "\n",
    "Adj. R-squared:   0.363\n",
    "\n",
    "There are some of the actual crops with a very high P Value I will drop some of these to see does it improve our accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - Linear Regression (Dropping all of the cluster group columns as the p value is above 0.5) backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the categorial dummie Crop values  Group features with a high p-value from the training model\n",
    "X_train4 = X_train3.drop(['crops_type_Cabbages and other brassicas','crops_type_Chillies and peppers, green','crops_type_Linseed','crops_type_Onions, dry','crops_type_Strawberries','crops_type_Whey, dry'], axis=1)\n",
    "mlr4 = sm.OLS(y_train, X_train4).fit()\n",
    "print(mlr4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4\n",
    "\n",
    "For our fourth, we see that the adjusted R2 has not decreased after dropping the group of crops with a high Value.  The accuracy with Linear is almost 40percent and not considered good however all of the P Values are well under 0.5 and this cannot be improved upon.\n",
    "\n",
    "Adj R2:    0.363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After looking at the results from model 4 I will keep this as our final model as all pvalues are under 0.5 infact almost at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Inflation Factors (VIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance inflation factors range from a value of 1.0 and upwards. The VIF helps us quantify the severity of multicollinearity in an OLS regression. The VIF value tells us how much larger the standard error increases compared to if that variable had 0 correlation to other independent predictor variables in your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to calculate and display VIF for each variable\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train4.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train4.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of the variables have relatively low VIF values with the exception of station id which is still below our cuttoff of 5.0 which is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above and interpreting them 1 = not correlated. Between 1 and 5 = moderately correlated. Greater than 5 = highly correlated. \n",
    "\n",
    "Moderately correlated \n",
    "59 \tcrops_type_Walnuts, with shell \t1.20\n",
    "14 \tcrops_type_Carrots and turnips \t1.19\n",
    "60 \tcrops_type_Watermelons \t1.10\n",
    "41 \tcrops_type_Oranges \t1.08\n",
    "28 \tcrops_type_Hazelnuts, with shell \t1.06\n",
    "\n",
    "We can see that  features 5 to 12 have a VIF of infinity this means there is perfect correlation.\n",
    "\n",
    "5 \tcrops_action_Yield \t87.30\n",
    "9 \tcrops_type_Barley \t61.10\n",
    "6 \tcrops_type_Apricots \t10.34\n",
    "7 \tcrops_type_Avocados \t10.09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions With Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create our final model for prediction using the final set of variables that we had in our final training model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop predictive variables to prepare final model\n",
    "X_test = sm.add_constant(X_test)\n",
    "X_test_1 = X_test[X_train4.columns] \n",
    "\n",
    "# fit data to linear regression for final model using test data\n",
    "mlr_test = sm.OLS(y_test, X_test_1).fit()\n",
    "\n",
    "# inspect X_test data\n",
    "X_test_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make predictions of the y variable which is bikes_available using our final test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the final model\n",
    "y_pred = mlr_test.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Predictions of Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our final model we will look a number of data points and visualizations. First we start off with a distribution plot of predicted y values subtracted from the actual y values from the test dataset. This will help us visualize the distribution of errors. We also take a look at a scatter plot of predicted y values vs actuals from the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot of predicted y values vs test y values\n",
    "sns.distplot((y_test - y_pred), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test,y_pred)\n",
    "fig.suptitle('y_test vs y_pred', fontsize=20)   \n",
    "plt.xlabel('y_test ', fontsize=18)                       \n",
    "plt.ylabel('y_pred', fontsize=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph above observing the spread for y_pred and Y_test there is a couple of outliers but for the majority we can say that there is infact a high linear correlation. The RMSE is the standard deviation of the prediction errors also known as residuals. This helps us understand how well the actual data fits our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## * NORMAL DISTRIBUTION OF ERROR (TEST - PREDICTED ) *\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot((y_test-y_pred),bins=50,hist_kws=dict(edgecolor=\"k\", linewidth=2));\n",
    "plt.xlabel('y_test', fontsize='15')\n",
    "plt.ylabel('y_pred', fontsize='15')\n",
    "plt.title(\"NORMAL DISTRIBUTION OF ERROR (TEST - PREDICTED )\")\n",
    "plt.savefig(\"lr1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LINEAR REGRESSION CURVE\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.regplot(y_test,y_pred,df_irishdata,color='purple')\n",
    "plt.xlabel(\"Y TEST\")\n",
    "plt.ylabel(\"Y_PRED\")\n",
    "plt.title(\"LINEAR REGRESSION CURVE\")\n",
    "plt.savefig(\"lr3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will look at the R Squared (R2), Adjusted R Squared (Adj R2), and Root Mean Square Error (RMSE) values for the predictions.\n",
    "\n",
    "The RMSE is the standard deviation of the prediction errors also known as residuals. This helps us understand how well the actual data fits our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# create Adjusted R2 Score\n",
    "p = len(X_test_1.columns)\n",
    "n = y_test.shape[0]\n",
    "adj_r2 = 1 - (1 - r2) * ((n - 1)/(n-p-1))\n",
    "\n",
    "# create RMSE score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "\n",
    "\n",
    "# print final model performance stats\n",
    "print(str(p) + \" Predictors in Test Set\")\n",
    "print(str(n) + \" Records in Test Set\")\n",
    "print(\"R2: \" + str(r2))\n",
    "print(\"Adj R2: \" + str(adj_r2))\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the R2 and Adj R2 values from the training dataset vs the test dataset.\n",
    "\n",
    "Training Dataset\n",
    "\n",
    "R2: 0.3868084340285938\n",
    "Adj R2: 0.37636683087280487\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Linear Model Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE: A metric that tells us the mean absolute difference between the predicted values and the actual values in a dataset. The lower the MAE, the better a model fits a dataset\n",
    "\n",
    "The lower the MSE the higher the accuracy of prediction as there would be excellent match between the actual and predicted data set\n",
    "\n",
    "RMSE: A metric that tells us the square root of the average squared difference between the predicted values and the actual values in a dataset. The lower the RMSE, the better a model fits a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the R2_Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Not using Stats Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_irishdata.drop('month', axis=1, inplace=True)\n",
    "df_irishdata.drop('country', axis=1, inplace=True)\n",
    "df_irishdata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_irishdata.drop('crop_measurement', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_irishdata = pd.get_dummies(df_irishdata, columns = ['crops_action','crops_type'],drop_first = True)\n",
    "\n",
    "\n",
    "# inspect bike_data df with added dummy variables\n",
    "df_irishdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_irishdata['crop_value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_irishdata.drop('crop_value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Multiple Linear Regression model on the Training set\n",
    "#This is the same code we used in Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "#Remember that we need to check our training results on the Test set but we can't plot a graph\n",
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2) #we display values with only 2 decimals after the comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the precision of the model or r^2\n",
    "print('The precision of the model is ')\n",
    "print(regressor.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build the equation\n",
    "print('a = ')\n",
    "print(regressor.coef_)\n",
    "\n",
    "print('The interception is: ')\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Diagnostics\n",
    "\n",
    "The aim was to confirm whether the underlying assumptions of linear regression have been met which includes linearity, homoscedasticity, indpendence and normality. An asssement of various plots and statistical testing would reveal the above.\n",
    "\n",
    "The Scale Location Plot would indicate whether (homoscedasticity or heterescedasticity) is present among residuals. The line should be roughly horizontal which would give an indication that the variance of the residuals is approximately the same for all the fitted values. Also, no patterns should be visual among the residuals. These two conditions if met would indicate homosedasticity, one of the underlying assumptions of linear regression.\n",
    "\n",
    "The Actual vs Fitted plot is used to compare both the predicted values and the actual values. Ideally, the individual points in this plot should be close to forming a 45 degree straight line, indicating that predicted values are close to the actual values.\n",
    "\n",
    "Residuals vs Fitted plot can utilized to identify non linearity, non - constant error variances and outliers. In this plot we would be looking for the residuals to be spread equaly around a horizontal line with no distinct patterns visible. This would further solidify homscedasticity.\n",
    "\n",
    "The Normal QQ Plot will indicate whether the residuals are following a normal distribution or whether they deviate from a normal distibution. In this plot we are looking for the residuals to follow close to the line, if they dieviate this indicates the residuals do not follow a nomral distribution.\n",
    "\n",
    "There are Statisitcal Measure's such as:\n",
    "\n",
    "The 'Breush Pogan Test' which is used to check for varaince equality. The resulting value returned will inidcate whether the homoscedacitiy/heteroscacity is present among residuals.\n",
    "\n",
    "The Durbin Watson Test could be utilized in this case to identify whether there is correlation present among the residuals. A value close to [2] reveals there is no correlation present. A value close to [0] reveals a strong positve correaltion is present and where values closer to [4] would indicate strong negative correlation being present.\n",
    "\n",
    "All of the above techniques and metrics help identify whether the underlying assumptions of linear regression have been met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first train our first ridge regression model by setting to 1. This is the equivalent of running ordinary least squares regression with no penalty. Note that Scikit confusingly names the lambda parameter as alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_irishdata.drop('month', axis=1, inplace=True)\n",
    "df_irishdata.drop('country', axis=1, inplace=True)\n",
    "df_irishdata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_irishdata.drop('crop_measurement', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataframe\n",
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.4f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 10\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification (binary) - Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_irishdata.drop('month', axis=1, inplace=True)\n",
    "df_irishdata.drop('country', axis=1, inplace=True)\n",
    "df_irishdata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_irishdata.drop('crop_measurement', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the mean of the target / dependant variable\n",
    "mean_cropval= df_irishdata['crop_value'].mean()\n",
    "print(mean_cropval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new feature called Average Crop Price to find out if the crop value is less than the cropaverage value\n",
    "81432.7401053934 1 or 0 if the crop value is higer than the average (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The affordable column now has a value of 1 whenever the price is < 81432.7401053934, and 0 otherwise.\n",
    "df_irishdata['average_crop_value'] = np.where(df_irishdata['crop_value'] < 181432.7401053934, 1, 0)\n",
    "df_irishdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irishdata['average_crop_value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data below in train test and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_irishdata, test_size=0.2, random_state=20, stratify=df_irishdata['average_crop_value'])\n",
    "df_train, df_dev = train_test_split(df_train, test_size=0.20, random_state=40)\n",
    "\n",
    "# ensure our dataset splits are of the % sizes we want\n",
    "total_size = len(df_train) + len(df_dev) + len(df_test)\n",
    "print(\"train:\", len(df_train), \"=>\", len(df_train) / total_size)\n",
    "print(\"dev:\", len(df_dev), \" =>\", len(df_dev) / total_size)\n",
    "print(\"test:\", len(df_test), \"=>\", len(df_test) / total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing 'average_crop_value' from the dataframes below and creating it as a seperate prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "x_train = df_train.drop(['crop_value', 'average_crop_value'], axis=1)\n",
    "y_train = pd.DataFrame(data=df_train['average_crop_value'], columns=[\"average_crop_value\"])\n",
    "\n",
    "# dev\n",
    "x_dev = df_dev.drop(['crop_value', 'average_crop_value'], axis=1)\n",
    "y_dev = pd.DataFrame(data=df_dev[\"average_crop_value\"], columns=[\"average_crop_value\"])\n",
    "\n",
    "# test\n",
    "x_test = df_test.drop(['crop_value', 'average_crop_value'], axis=1)\n",
    "y_test = pd.DataFrame(data=df_test['average_crop_value'], columns=[\"average_crop_value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and cleaning based on the training set, x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in x_train.columns:\n",
    "    print(col, \":\", np.sum([x_train[col].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the null values\n",
    "df_irishdata.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape after dropping\n",
    "print(\"The shape of the data after dropping the null values is: \",df_irishdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape after dropping\n",
    "print(\"The shape of the data after dropping the null values is: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns of our training data all have reasonable summary statistics. None of the min's or max's are a cause for concern, and we have no reason to assert a certain distribution of values. Since all the feature values are within reasonable ranges, and there are no missing values (NaNs) remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in x_train.columns:\n",
    "   print(col, \":\", np.sum([x_train[col].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[col for col in x_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot is great for this, as it provides us with a high-level picture of how every pair of features correlates. If any subplot of features depicts a linear relationship (i.e., a clear, concise path with mass concentrated together), then we can assume there exists some collinearity -- that the two features overlap in what they are capturing and that they are not independent from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(x_train, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicing with maximum likelihood estimation (MLE) using the training set MLE for 𝑦, where 𝑦∈{0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_y = y_train['average_crop_value'].value_counts().idxmax()\n",
    "dev_accuracy = y_dev['average_crop_value'].value_counts()[mle_y] / len(y_dev['average_crop_value'])\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Predictions using logistic Regression are 83percent percent so this is a high level of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_irishdata.drop('month', axis=1, inplace=True)\n",
    "df_irishdata.drop('country', axis=1, inplace=True)\n",
    "df_irishdata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_irishdata.drop('crop_measurement', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_irishdata = pd.get_dummies(df_irishdata, columns = ['crops_action','crops_type'],drop_first = True)\n",
    "\n",
    "\n",
    "# inspect bike_data df with added dummy variables\n",
    "df_irishdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataframe after dropping columns\n",
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "\n",
    "X = df_irishdata.iloc[:, 1:74].values\n",
    "y = df_irishdata.iloc[:, 4].values\n",
    "# set the independent predictor variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataaset into Training and Testing Data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random state is 0 and test size if 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=0)#70 Training and 30 Test\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we split the data, the next step is to scale our dataset so the extreme values will not have too much effect on the prediction of our model. Notice that we are only scaling the input values, not the output ones. Now, our data is ready for training the model using a decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing standard scalling method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# providing the inputs for the scalling purpose\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "  \n",
    "# create a regressor object\n",
    "regressor = DecisionTreeRegressor(random_state = 0) \n",
    "  \n",
    "# fit the regressor with X and Y data\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the outputs\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Decision Tree (Training Model)using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing the plot tree method\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# output size of decision tree\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "# providing the training dataset\n",
    "clf = clf.fit(X_train, y_train)\n",
    "plot_tree(clf, filled=True)\n",
    "plt.title(\"Decision tree training for training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Decision Tree (Testing Dataset)using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the plot tree method\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# output size of decision tree\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "# providing the training dataset\n",
    "clf = clf.fit(X_test, y_test)\n",
    "plot_tree(clf, filled=True)\n",
    "plt.title(\"Decision tree training for training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Text based Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the tree\n",
    "from sklearn import tree\n",
    "\n",
    "# text based tree\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Accuracy of the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the regressor correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicting the values of test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\"Classification report - \\n\", classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_pred, y_test)\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('confusion.png')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the True Positive, True Negative, False Positive, and False Negative values by using python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining a function which takes acutal and pred values\n",
    "def confusion_values(y_actual, y_pred):\n",
    "\n",
    "    # initializing the values with zero value\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    # iterating through the values using a for loop\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "           TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "           TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "           FN += 1\n",
    "\n",
    "    # printing the values\n",
    "    print(\"True Positive: \", TP)\n",
    "    print(\"False Positive:\", FP)\n",
    "    print(\"True Negative: \", TN)\n",
    "    print(\"False Negative: \", FN)\n",
    "\n",
    "# calling the function and passing in the parameters from the results of the iterations in the for loop\n",
    "confusion_values(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive: A true positive is an outcome where the model correctly predicts the positive class.\n",
    "True Negative: A true negative is an outcome where the model correctly predicts the negative class.\n",
    "False Positive: A false negative is an outcome where the model incorrectly predicts the positive class.\n",
    "False Negative: A false negative is an outcome where the model incorrectly predicts the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required module and methods\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(f'Accuracy-score: {accuracy_score(y_test, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "\n",
    "models = [LinearRegression(),\n",
    "         Ridge(),\n",
    "         HuberRegressor(),\n",
    "         ElasticNetCV(),\n",
    "         DecisionTreeRegressor(),\n",
    "         RandomForestRegressor(),\n",
    "         ExtraTreesRegressor(),\n",
    "         GradientBoostingRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "def train(model):\n",
    "    kfold = model_selection.KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "    pred = model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    cv_score = pred.mean()\n",
    "    print('Model:',model)\n",
    "    print('CV score:', abs(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "for model in models:\n",
    "    train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_irishdata = pd.read_csv('df_irishmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_irishdata.drop('month', axis=1, inplace=True)\n",
    "df_irishdata.drop('country', axis=1, inplace=True)\n",
    "df_irishdata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_irishdata.drop('crop_measurement', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_irishdata = pd.get_dummies(df_irishdata, columns = ['crops_action','crops_type'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_irishdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_irishdata.iloc[:, 1:75].values\n",
    "y = df_irishdata.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Random Forest Regression model on the whole dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model Performance\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Brazil Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "brazilagri_crops = pd.read_csv('brazil/FAOSTAT_brazil_5-1-2022(4).csv')\n",
    "brazilagri_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the last 5 rows of the dataframe\n",
    "brazilagri_crops.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of variables min max and median and quartiles of numeric features\n",
    "brazilagri_crops.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataset information and datatypes\n",
    "brazilagri_crops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan or missing values from multiple columns before I merge\n",
    "brazilagri_crops =  brazilagri_crops.dropna()\n",
    "brazilagri_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the irish crops dataset before we merge\n",
    "brazilagri_crops.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am concentrating on crops so I have removed livestock and non crops that appear under the column item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows of all rows under item that do not contain crops but contain livestock etc.\n",
    "brazilagri_crops= brazilagri_crops[brazilagri_crops[\"Item\"].str.contains(\"Asses|Beer of barley|Cattle|skimmed cow milk|Cheese, whole cow milk|Chickens|Cream fresh|Ducks|Eggs, hen, in shell|Fat, cattle|Fat, pigs|Fat, sheep|Geese and guinea fowls|Goats|Hides, cattle, fresh|Honey, natural|Hops|Horses|Lard|Margarine, short|Meat nes|Meat, cattle|Meat, chicken|Meat, duck Meat, goose and guinea fowl|Meat, horse|Meat, pig|Meat, sheep|Meat, turkey|Milk, skimmed cow|Milk, skimmed dried|Milk, whole condensed|Milk, whole dried|Milk, whole evaporated|Milk, whole fresh cow|Mules|Offals, edible, cattle|Offals, horses|Offals, pigs, edible|Offals, sheep,edible|Oil, coconut (copra)|Oil, cottonseed|Oil, groundnut|Oil, linseed|Oil, palm kernel|Oil, rapeseed|Oil, soybean|Pigs|Rapeseed|Sheep|Skins, sheep, fresh|Sugar Raw Centrifugal|Tallow|Turkeys|fresh nes|Whey, condensed|Wool, greasy|Molasses|Almonds, with shell|Buffaloes|Cottonseed|Fat, goats|Hemp tow waste|Meat, goat|Milk, whole fresh buffalo|Milk, whole fresh goat|Milk, whole fresh sheep|Offals, edible, goats|paddy (rice milled equivalent)|Skins, goat, fresh|Sorghum|Tobacco, unmanufactured|Triticale|Meat, duck|Meat, goose and guinea fowl\") == False]\n",
    "# display\n",
    "brazilagri_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Brazil Consumer dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "brazilagri_consumer = pd.read_csv('brazil/FAOSTATbrazil_data_5-1-2022.csv')\n",
    "brazilagri_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the last 4 observations of the dataframe by default\n",
    "brazilagri_consumer.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of variables\n",
    "brazilagri_consumer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect new dataframe for dtype info size etc.\n",
    "brazilagri_consumer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the shape of the new merged dataframe\n",
    "brazilagri_consumer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Assigning the columns from the Brazil Consumer dataset to be used in a left Join on year with the crops dataset\n",
    "df_brazilagri_consumer = brazilagri_consumer[['Year','Value','Months']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging both datasets using a left Inner Join on Year and year columnn and assiging the result to a new \n",
    "#dataframe called brazildata\n",
    "brazildata = pd.merge(brazilagri_crops,df_brazilagri_consumer,left_on=\"Year\",right_on=\"Year\", how =\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataframe\n",
    "brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the last 5 rows of the new dataframe after the merge\n",
    "brazildata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect description of variables\n",
    "brazildata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect new dataframe for dtype info size etc.\n",
    "brazildata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the shape of the new merged dataframe\n",
    "brazildata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all year type to ints to ensure there are no strings for the years greater than 2009 and this covers the years for this data 2010 to 2020\n",
    "#The second 3rd line we we ensure that year is ints again sometimes after sorting or merging they can turn into strings\n",
    "\n",
    "df_brazildata = brazildata[brazildata['Year'].astype('int64') > 2009]\n",
    "df_brazildata = df_brazildata.sort_values(by=['Year','Value_x','Value_y'])\n",
    "df_brazildata['Year'] = df_brazildata['Year'].astype('int64')\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataset\n",
    "df_brazildata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename our columns to be more descriptive names for some and ensure all of the uppercase ones are \n",
    "#lowercase to keep in line with naming conventions\n",
    "df_brazildata = df_brazildata.rename(columns={'Domain':'crops_domain',\n",
    "                       'Year':'year',\n",
    "                       'Item':'crops_type',\n",
    "                       'Value_x':'crop_value',\n",
    "                       'Element':'crops_action',\n",
    "                       'Unit':'crop_measurement',\n",
    "                       'Area':'country',\n",
    "                       'Months':'months',\n",
    "                       'Value_y':'consumer_value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataframe\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder Columns\n",
    "\n",
    "brazilcolumns = [\"year\",\"country\",\"months\",\"crop_measurement\",\"crops_type\",\"crops_action\",\"crop_value\",\"consumer_value\"]\n",
    "df_brazildata = df_brazildata.reindex(columns = brazilcolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for NAN values if any\n",
    "\n",
    "df_brazildata.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sum of null records\n",
    "df_brazildata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking dataframe for missing data\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 5))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(df_brazildata.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see we have no missing values or Nans within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.box(df_brazildata, y='consumer_value')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#\n",
    "# Box plot\n",
    "#\n",
    "sns.boxplot(df_brazildata.crop_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable selling price is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_brazildata['consumer_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the data is partially skewed and that the extreme minimum is an outlier. whisker plot is slightly above and past the mean You may also notice that the extreme minimum is pretty far away from the box, and the box is located farther to the right. This means that the data is partially skewed and that the extreme minimum is an outlier.\n",
    "\n",
    "But as this is prices I would not worry, however if this were 10 bathrooms when the norm is 2 maybe 3 I would be removing them as it would affect my results in the models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Distribution plot\n",
    "#\n",
    "sns.distplot(df_brazildata.crop_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see from the above graph that the variable cropvalue is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check rows where dependent variable is equal to zero\n",
    "df_brazildata.loc[df_brazildata['crop_value']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Variable Crop Value and Consumer Value I dont want any zeros in either for the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see for the crop value column there are over 612 rows with the value 0 I we want to find the mean value of the column\n",
    "print(df_brazildata['crop_value'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all rows with zero for the column crop_value replace them with the average value listed above and below.\n",
    "df_brazildata['crop_value'] = df_brazildata['crop_value'].replace(0, 5333022.564655173) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 612 rows for the column crop_value that are null I am going to replace them too with the mean average value for that feature\n",
    "# Replace NaNs in column crop_value with the\n",
    "# mean of values in the same column\n",
    "mean_value=df_brazildata['crop_value'].mean()\n",
    "df_brazildata['crop_value'].fillna(value=mean_value, inplace=True)\n",
    "print('Updated Dataframe:')\n",
    "print(df_brazildata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check rows where dependent variable is equal to zero\n",
    "df_brazildata.loc[df_brazildata['consumer_value']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumer Value has no zero values to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sum of null records again it should be 0 this time\n",
    "\n",
    "df_brazildata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check cardinality\n",
    "\n",
    "v_cardinality = df_brazildata.nunique()\n",
    "\n",
    "print(v_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot cardinality using a barplot\n",
    "\n",
    "df_brazildata.nunique().plot.bar(figsize=(12,6))\n",
    "\n",
    "plt.ylabel('Number of unique categories')\n",
    "\n",
    "plt.xlabel('Variables')\n",
    "\n",
    "plt.title('Cardinality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape and size\n",
    "print(\"Shape of the dataset is : \",df_brazildata.shape)\n",
    "print(\"Size of the dataset is : \",df_brazildata.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the crop price from 2010 to 2020\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='year', y='crop_value', data=df_brazildata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the consumer value from 2010 to 2020\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='year', y='consumer_value', data=df_brazildata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_brazildata, x='year', y='crop_value',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_brazildata, x='year', y='consumer_value',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two arrays to compare coefficients\n",
    "consumervalue=np.array(df_brazildata[\"consumer_value\"])\n",
    "cropvalue=np.array(df_brazildata[\"crop_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to work out coefficients between consumer value and crop value\n",
    "np.corrcoef (consumervalue, cropvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result tells us that there is a high correlation between consumer value and cropvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a scatterplot to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Were creating two variables for our min and max values for the slider\n",
    "xmin,xmax=min(df_brazildata[\"year\"]), max(df_brazildata[\"year\"])\n",
    "ymin,ymax=min(df_brazildata[\"crop_value\"]), max(df_brazildata[\"crop_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using year as the animation frame, and as we go through each year the values will change want to group it by month\n",
    "#The color will be the crops type each crop will have a different color, using hover for the year to give us info\n",
    "#Split up the sections by usi by crops Action (yield, Harvest and Productions) \n",
    "\n",
    "fig= px.scatter(df_brazildata, x=\"year\", y=\"crop_value\", animation_frame =\"year\",\n",
    "               animation_group=\"months\",color=\"crops_type\", hover_name=\"months\", \n",
    "               facet_col=\"crops_action\", width=1580, height=400, log_x=True, size_max=10000,\n",
    "               range_x=[xmin,xmax],range_y=[ymin,ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics for the Target features: crop value & consumervalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect the dataframe for outliers will display the min,max mean and inner and outer quariles for all features\n",
    "df_brazildata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for the target feature crop_value\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x= df_brazildata['crop_value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mean of the target feature\n",
    "mean_cropvalue=  df_brazildata['crop_value'].mean()\n",
    "print(mean_cropvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the median of the target feature\n",
    "median = df_brazildata['crop_value'].median()\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mode of the target feature\n",
    "mode = df_brazildata['crop_value'].mode()\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#histplot to display the distribution of crap _value\n",
    "ax = sns.histplot(df_brazildata.crop_value)\n",
    "ax.set(xlabel='Crop Price', ylabel='Cropvalue', title ='Cropvalue Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above graph that the variable selling price is slightly right or postively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot to depict Box plot Crop Value by Crop Action\n",
    "sns.boxplot(x = 'crops_action', y ='crop_value', data=df_brazildata, hue = 'crops_action')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title('Box plot Crop Value by Crop Action', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows of all rows under crops action that do not contain Yield only is what I am intersested in \n",
    "df_irishdata= df_brazildata[df_brazildata[\"crops_action\"].str.contains(\"Production|Area harvested|Laying|Producing Animals/Slaughtered|Stocks\") == False]\n",
    "# display\n",
    "df_brazildata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplot to display the distribution of crap _value in Red \n",
    "ax = sns.histplot( df_brazildata.crop_value, kde=True, color ='red')\n",
    "ax.set(xlabel='Crop Value', ylabel='Crop Price', title ='Crop Prices Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "ax = sns.barplot(x=\"year\", y=\"crop_value\", data=df_brazildata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#boxplot for the feature rain\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x= df_brazildata['consumer_value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mean of the target feature\n",
    "mean_consumervalue=  df_brazildata['consumer_value'].mean()\n",
    "print(mean_consumervalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the median of the target feature\n",
    "median = df_brazildata['consumer_value'].median()\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mode of the target feature\n",
    "mode = df_brazildata['consumer_value'].mode()\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#histplot to display the distribution of consumer _value\n",
    "ax = sns.histplot(df_brazildata.consumer_value)\n",
    "ax.set(xlabel='Consumer Price', ylabel='consumervalue', title ='Consumervalue Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Box plot to depict Box plot Crop Value by Crop Action\n",
    "sns.boxplot(x = 'crops_action', y ='consumer_value', data=df_brazildata, hue = 'crops_action')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title('Box plot Consumer Value by Crop Action', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list the datatypes of all columns we can see there are alot of objects later I will change these to onehotencoding for the linear regression\n",
    "df_brazildata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk Test - Normality Test - Target crop_value\n",
    "\n",
    "Tests whether a data sample has a Gaussian distribution. examines if a variable is normally distributed in a population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk Normality Test\n",
    "from scipy.stats import shapiro\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "    H0: the sample has a Gaussian distribution.\n",
    "    H1: the sample does not have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value of the Shapiro-Wilk Test is greater than 0.05, the data is normal. If it is below 0.05, the data significantly deviate from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro-Wilk Test - Normality Test - Target Consumer_value\n",
    "\n",
    "Tests whether a data sample has a Gaussian distribution. Examines if a variable is normally distributed in a population\n",
    "Crop Value\n",
    "Consumer Value\n",
    "Crops_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapiro-Wilk Normality Test\n",
    "from scipy.stats import shapiro\n",
    "consumer_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(consumer_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "Observations in each sample are independent and identically distributed (iid).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "H0: the sample has a Gaussian distribution.\n",
    "H1: the sample does not have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Shapiro-Wilk Normality Test\n",
    "from scipy.stats import shapiro\n",
    "crop_type = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = shapiro(crop_type)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the result is >0.05 the result is normal, the data significantly deviate from a normal distribution if <0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import useful library\n",
    "import numpy as np\n",
    "from numpy.random import poisson\n",
    "from numpy.random import seed\n",
    "from scipy.stats import shapiro\n",
    "from numpy.random import randn\n",
    "  \n",
    "seed(0)\n",
    "# Create data\n",
    "crops_action = poisson(5, 200)\n",
    "  \n",
    "# conduct the  Shapiro-Wilk Test\n",
    "shapiro(crops_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the above example, the p-value is 0.0001 which is less than the alpha(0.5) then we reject the null hypothesis i.e. we have sufficient evidence to say that sample does not come from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson’s Correlation Coefficient\n",
    "\n",
    "Tests whether two samples have a linear relationship.\n",
    "\n",
    "Crop_Value & Crop Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_action= [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = pearsonr(crop_value, crop_action)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson’s Correlation Coefficient\n",
    "\n",
    "Tests whether two samples have a linear relationship.\n",
    "\n",
    "Crop_Value & consumer_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "consumer_value= [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = pearsonr(crop_value, consumer_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "H0: the two samples are independent.\n",
    "H1: there is a dependency between the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman’s Rank Correlation\n",
    "\n",
    "Tests whether two samples have a monotonic relationship.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample can be ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "crop_measurement = [0.353, 3.517, 0.125, -7.545, -0.555, -1.536, 3.350, -1.578, -3.537, -1.579]\n",
    "stat, p = spearmanr(crop_value, crop_measurement)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D’Agostino’s K^2 Test - Normality Test\n",
    "\n",
    "Tests whether a data sample has a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "crop_value = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "stat, p = normaltest(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably Gaussian')\n",
    "else:\n",
    "\tprint('Probably not Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "    H0: the sample has a Gaussian distribution.\n",
    "    H1: the sample does not have a Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Chi-square test on a df_brazil dataset on the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "df_brazildata = pandas.read_csv(\"brazil/FAOSTAT_brazil_5-1-2022(4).csv\")\n",
    "categorical_col = ['Domain', 'Area', 'Item', 'Element','Unit']\n",
    "print(categorical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the crosstab() function to create a contingency table of the two selected variables to work on ‘crops_typey’ and ‘crops_action’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisqt = pandas.crosstab(df_brazildata.Domain, df_brazildata.Element, margins=True)\n",
    "print(chisqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply the chi2_contingency() function on the table and get the statistics, p-value and degree of freedom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency \n",
    "import numpy as np\n",
    "chisqt = pandas.crosstab(df_brazildata.Domain, df_brazildata.Element, margins=True)\n",
    "value = np.array([chisqt.iloc[0][0:5].values,\n",
    "                  chisqt.iloc[1][0:5].values])\n",
    "print(chi2_contingency(value)[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, 1.0 is the p-value, 0.0 is the statistical value and 4 is the degree of freedom. As the p-value is greater than 0.05, we accept the NULL hypothesis and assume that the variables ‘Domain’ and ‘Element’ are independent of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Squared Test\n",
    "\n",
    "Tests whether two categorical variables are related or independent.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations used in the calculation of the contingency table are independent.\n",
    "    25 or more examples in each cell of the contingency table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Chi-Squared Test\n",
    "from scipy.stats import chi2_contingency\n",
    "crop_value = [[10, 20, 30],[6,  9,  17]]\n",
    "stat, p, dof, expected = chi2_contingency(crop_value)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably independent')\n",
    "else:\n",
    "\tprint('Probably dependent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE can see that the P - value for crop value is greater than 0.5 and is probably independant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "    H0: the two samples are independent.\n",
    "    H1: there is a dependency between the samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student’s t-test\n",
    "\n",
    "Tests whether the means of two independent samples are significantly different.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Student's t-test\n",
    "from scipy.stats import ttest_ind\n",
    "max_airtemp = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "min_airtemp = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(max_airtemp, min_airtemp)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "\n",
    "    H0: the means of the samples are equal.\n",
    "    H1: the means of the samples are unequal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Statistical Hypothesis Tests\n",
    "\n",
    "This section lists statistical tests that you can use to compare data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired Student’s t-test\n",
    "\n",
    "Tests whether the means of two paired samples are significantly different.\n",
    "\n",
    "Assumptions\n",
    "\n",
    "    Observations in each sample are independent and identically distributed (iid).\n",
    "    Observations in each sample are normally distributed.\n",
    "    Observations in each sample have the same variance.\n",
    "    Observations across each sample are paired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Student's t-test\n",
    "from scipy.stats import ttest_ind\n",
    "rain = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "year = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "stat, p = ttest_ind(rain, year)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interpretation\n",
    "\n",
    "    H0: the means of the samples are equal.\n",
    "    H1: the means of the samples are unequal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE can see that the P value for Year is greater than 0.5 and is therefore of the same distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.regplot(x = df_brazildata[\"year\"], y = df_brazildata[\"crop_value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.regplot(x = df_brazildata[\"year\"], y = df_brazildata[\"consumer_value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the pearson correlation coefficients. This helps us understand the extent to which two variables are correlated. We will be able to see both the strength of the correlation as well as the direction and use that to make a decision on the exclusion of predictive variables that display multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create correlation matrix displaying pearson correlation coefficients for all variables\n",
    "corr_matrix = df_brazildata.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visually examine the relationships between the measurable variables via a scatterplot using a randomly selected sample size of 1,000. We also see the histograms for each of the measurable variables displayed across the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pair grid with histograms and scatterplots using seaborn pairgrid\n",
    "irish_data_sample =  df_brazildata.sample(1000)\n",
    "p = sns.PairGrid(data=irish_data_sample, vars=[\"year\",\"country\",\"months\",\"crop_measurement\",\"crops_type\",\"crops_action\",\"crop_value\",\"consumer_value\"])\n",
    "p.map_diag(plt.hist)\n",
    "p.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a roughly normal distribution for available_bike_stands. A decision will need to be made as to which should be used when we create the linear regression model. crops_type will be the Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create point plot using seaborn pointplot to display Crop Value by month for each year from 2010 to 2020\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_brazildata, x='year', y='crop_value', ci=\"sd\", hue='months', palette='gist_rainbow_r')\n",
    "ax1.set(title='Crop value by month for each year from 2010 to 2020')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create point plot comparing Crop Value and the type of crop action for each year from 2010 to 2020 using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_brazildata, x='year', y='crop_value', ci=\"sd\", hue='crops_action', palette='YlGnBu')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax1.set(title='Crop Value and the type of crop action for each year from 2010 to 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create point plot Count of the type of crops and value for each year from 2010 to 2020 using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_brazildata, x='year', y='crop_value', ci=\"sd\", hue='crops_type', palette='prism_r')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax1.set(title='Count of the type of crops and value for each year from 2010 to 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create point plot comparing consumer value by month over the decade using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_brazildata, x='year', y='consumer_value', ci=\"sd\", hue='months', palette='gist_rainbow_r')\n",
    "ax1.set(title='consumer value by month over the decade')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the start to the end of the decade the highest month for consumer value was December this could be when its reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Crop value by crop action for each year over the decade using seaborn pointplot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.pointplot(data=df_brazildata, x='year', y='crop_value', hue='crops_action', ax=ax)\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax.set(title='Crop value by crop action for each year over the decade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production was clearly the highest in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the crop price from 2010 to 2020\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='year', y='crop_value',data=df_brazildata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the rain for each year\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x='year', y='consumer_value',data=df_brazildata)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create box plots for time related variables using seaborn boxplot\n",
    "figure, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4)\n",
    "figure.set_size_inches(24, 8)\n",
    "\n",
    "sns.boxplot(data=df_brazildata, x='year', y='crop_value', ax=ax1)\n",
    "sns.boxplot(data=df_brazildata, x='months', y='crop_value', ax=ax2)\n",
    "sns.boxplot(data=df_brazildata, x='year', y='consumer_value', ax=ax3)\n",
    "sns.boxplot(data=df_brazildata, x='months', y='consumer_value', ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seaborn distplot used to depict the distribution of cropvalue and the Theoritical quantiles of that feature\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,6))\n",
    "sns.distplot(df_brazildata['crop_value'], ax=ax1)\n",
    "ax1.set(title='Distribution of the crop_value')\n",
    "qqplot(df_brazildata['crop_value'], ax=ax2, line='s')\n",
    "ax2.set(title='Theoritical quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seaborn distplot used to depict the distribution of max_airtemp and the Theoritical quantiles of grass_minimumtemp\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2, figsize=(20,6))\n",
    "sns.distplot(df_brazildata['consumer_value'], ax=ax1)\n",
    "ax1.set(title='Distribution of the consumer_value')\n",
    "qqplot(df_irishdata['consumer_value'], ax=ax2, line='s')\n",
    "ax2.set(title='Theoritical quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Coorelation Matrix to compare the relationships and correlation between features\n",
    "corr = df_brazildata.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, annot_kws={'size':15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1 indicates a perfectly negative linear correlation between two variables. 0 indicates no linear correlation between two variables. 1 indicates a perfectly positive linear correlation between two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using a seaborn relplot to plot 3 data points to see the relationship between 2 variables using my own color pallete hue is hour time of day\n",
    "sns.relplot(x=\"year\", y=\"crop_value\", hue=\"months\", palette=\"ch:r=-.5,l=.75\", data=df_brazildata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using plotly to display barchart Value of crops produced by Ireland each year crops type action and month displayed\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_brazildata, x='year', y='crop_value',\n",
    "                hover_data=['crops_type','months'], color='crops_action',\n",
    "                labels={'country':'Value of crops produced by Ireland each year'}, height=400, title='Value of crops produced by Ireland each year')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below the Csv after been explored and cleaned has now been saved so as I can just read this in for my models going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the data has been explored and cleaned saving it to be used for the model\n",
    "df_brazildata.to_csv('df_brazilmodels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in df_brazilmodels.csv into the df_brazildata frame for linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_brazildata.drop(columns=[\"Unnamed: 0\",\"country\"],axis=1,inplace=True)\n",
    "df_brazildata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_brazildata = pd.get_dummies(df_brazildata, columns = ['crops_action','crops_type','months','crop_measurement'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect bike_data df with added dummy variables\n",
    "df_brazildata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dummy variables we want to inspect the new full list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for Nans\n",
    "df_brazildata.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Multiple Regression and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we will create a multiple regression using all of the independant features to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_brazildata['crop_value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_brazildata.drop('crop_value', axis=1)\n",
    "\n",
    "# split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)# 70% training and 30% test\n",
    "X_train = sm.add_constant(X_train)\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Linear Regression using all independant variables/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fit data to linear regression\n",
    "mlr1 = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# view OLS regression results\n",
    "print(mlr1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared:                       0.352\n",
    "Model:                            OLS   Adj. R-squared:                  0.350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Linear Regression (Dropping consumer_value  well over 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the features below with a high p-value from the training model\n",
    "X_train2 = X_train.drop(['consumer_value'], axis=1)\n",
    "mlr2 = sm.OLS(y_train, X_train2).fit()\n",
    "print(mlr2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dep. Variable:             crop_value   R-squared:                       0.352\n",
    "Model:                            OLS   Adj. R-squared:                  0.350\n",
    "        NO improvement after dropping consumer_value some of the crops have a very high P- Value I will drop some of those to \n",
    "        see if there is improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Linear Regression (Dropping all of the hour columns) backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the Temperature features with a high p-value from the training model\n",
    "X_train3 = X_train2.drop(['crops_type_Avocados','crops_type_Barley','crops_type_Bastfibres, other','crops_type_Broad beans, horse beans, dry'], axis=1)\n",
    "mlr3 = sm.OLS(y_train, X_train3).fit()\n",
    "print(mlr3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:             crop_value   R-squared:                       0.352\n",
    "Model:                            OLS   Adj. R-squared:                  0.351 adj Rsquared slightly improved\n",
    "        \n",
    "        Going to drop further crop types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - Linear Regression (Dropping all of the cluster group columns as the p value is above 0.5) backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping all of the categorial dummie Crop values  Group features with a high p-value from the training model\n",
    "X_train4 = X_train3.drop(['crops_type_Buckwheat','crops_type_Cashew nuts, with shell','crops_type_Cashewapple','crops_type_Castor oil seed','crops_type_Cocoa, beans','crops_type_Garlic','crops_type_Grapefruit (inc. pomelos)','crops_type_Jute','crops_type_Nuts nes'], axis=1)\n",
    "mlr4 = sm.OLS(y_train, X_train4).fit()\n",
    "print(mlr4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:             crop_value   R-squared:                       0.352\n",
    "Model:                            OLS   Adj. R-squared:                  0.351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the results from model 4 I will keep this as our final model as all pvalues are under 0.5 infact almost at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Inflation Factors (VIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance inflation factors range from a value of 1.0 and upwards. The VIF helps us quantify the severity of multicollinearity in an OLS regression. The VIF value tells us how much larger the standard error increases compared to if that variable had 0 correlation to other independent predictor variables in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to calculate and display VIF for each variable\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train4.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train4.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above and interpreting them 1 = not correlated. Between 1 and 5 = moderately correlated. Greater than 5 = highly correlated. \n",
    "We can see 55,48,1 and 2 are NOT correlated and the top ones have a value of inf infinitity excellent perfect correlation \n",
    "this is for some of the crops and crop actions such as yield this would make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions With Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create our final model for prediction using the final set of variables that we had in our final training model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop predictive variables to prepare final model\n",
    "X_test = sm.add_constant(X_test)\n",
    "X_test_1 = X_test[X_train4.columns] \n",
    "\n",
    "# fit data to linear regression for final model using test data\n",
    "mlr_test = sm.OLS(y_test, X_test_1).fit()\n",
    "\n",
    "# inspect X_test data\n",
    "X_test_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make predictions of the y variable which is crops_value using our final test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the final model\n",
    "y_pred = mlr_test.predict(X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Predictions of Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our final model we will look a number of data points and visualizations. First we start off with a distribution plot of predicted y values subtracted from the actual y values from the test dataset. This will help us visualize the distribution of errors. We also take a look at a scatter plot of predicted y values vs actuals from the test data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution plot of predicted y values vs test y values\n",
    "sns.distplot((y_test - y_pred), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig = plt.figure()\n",
    "plt.scatter(y_test,y_pred)\n",
    "fig.suptitle('y_test vs y_pred', fontsize=20)   \n",
    "plt.xlabel('y_test ', fontsize=18)                       \n",
    "plt.ylabel('y_pred', fontsize=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph above observing the spread for y_pred and Y_test there is a couple of outliers but for the majority we can say that there is infact a moderate linear correlation. The RMSE is the standard deviation of the prediction errors also known as residuals. This helps us understand how well the actual data fits our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## * NORMAL DISTRIBUTION OF ERROR (TEST - PREDICTED ) *\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot((y_test-y_pred),bins=50,hist_kws=dict(edgecolor=\"k\", linewidth=2));\n",
    "plt.xlabel('y_test', fontsize='15')\n",
    "plt.ylabel('y_pred', fontsize='15')\n",
    "plt.title(\"NORMAL DISTRIBUTION OF ERROR (TEST - PREDICTED )\")\n",
    "plt.savefig(\"lr1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LINEAR REGRESSION CURVE\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.regplot(y_test,y_pred, df_brazildata,color='purple')\n",
    "plt.xlabel(\"Y TEST\")\n",
    "plt.ylabel(\"Y_PRED\")\n",
    "plt.title(\"LINEAR REGRESSION CURVE\")\n",
    "plt.savefig(\"lr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# create Adjusted R2 Score\n",
    "p = len(X_test_1.columns)\n",
    "n = y_test.shape[0]\n",
    "adj_r2 = 1 - (1 - r2) * ((n - 1)/(n-p-1))\n",
    "\n",
    "# create RMSE score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "\n",
    "\n",
    "# print final model performance stats\n",
    "print(str(p) + \" Predictors in Test Set\")\n",
    "print(str(n) + \" Records in Test Set\")\n",
    "print(\"R2: \" + str(r2))\n",
    "print(\"Adj R2: \" + str(adj_r2))\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the R2 and Adj R2 values from the training dataset vs the test dataset.\n",
    "\n",
    "Test Dataset\n",
    "R2: 0.37672347857732547\n",
    "Adj R2: 0.3743512322017184\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Linear Model Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE: A metric that tells us the mean absolute difference between the predicted values and the actual values in a dataset. The lower the MAE, the better a model fits a dataset\n",
    "\n",
    "The lower the MSE the higher the accuracy of prediction as there would be excellent match between the actual and predicted data set\n",
    "\n",
    "RMSE: A metric that tells us the square root of the average squared difference between the predicted values and the actual values in a dataset. The lower the RMSE, the better a model fits a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the R2_Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Not using Stats Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_brazildata.drop(columns=[\"Unnamed: 0\",\"country\"],axis=1,inplace=True)\n",
    "df_brazildata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_brazildata = pd.get_dummies(df_brazildata, columns = ['crops_action','crops_type','months','crop_measurement'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_brazildata['crop_value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_brazildata.drop('crop_value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Multiple Linear Regression model on the Training set\n",
    "#This is the same code we used in Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "#Remember that we need to check our training results on the Test set but we can't plot a graph\n",
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2) #we display values with only 2 decimals after the comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the precision of the model or r^2\n",
    "print('The precision of the model is ')\n",
    "print(regressor.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We build the equation\n",
    "print('a = ')\n",
    "print(regressor.coef_)\n",
    "\n",
    "print('The interception is: ')\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first train our first ridge regression model by setting to 1. This is the equivalent of running ordinary least squares regression with no penalty. Note that Scikit confusingly names the lambda parameter as alpha.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_brazildata.drop(columns=[\"Unnamed: 0\",\"country\"],axis=1,inplace=True)\n",
    "df_brazildata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_brazildata = pd.get_dummies(df_brazildata, columns = ['crops_action','crops_type','months','crop_measurement'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.4f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 10\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 3 columns from our regression\n",
    "df_brazildata.drop('months', axis=1, inplace=True)\n",
    "df_brazildata.drop('country', axis=1, inplace=True)\n",
    "df_brazildata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_brazildata.drop('crop_measurement', axis=1, inplace=True)\n",
    "df_brazildata.drop('crops_action', axis=1, inplace=True)\n",
    "df_brazildata.drop('crops_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# inspect brazil data df with added dummy variables\n",
    "df_brazildata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "\n",
    "X = df_brazildata.iloc[:, 0:3].values\n",
    "y = df_brazildata.iloc[:, 2].values\n",
    "# set the independent predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Regression to the dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataaset into Training and Testing Data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random state is 0 and test size if 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30, random_state=0)#70 Training and 30 Test\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we split the data, the next step is to scale our dataset so the extreme values will not have too much effect on the prediction of our model. Notice that we are only scaling the input values, not the output ones. Now, our data is ready for training the model using a decision tree algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing standard scalling method from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# providing the inputs for the scalling purpose\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "  \n",
    "# create a regressor object\n",
    "regressor = DecisionTreeRegressor(random_state = 0) \n",
    "  \n",
    "# fit the regressor with X and Y data\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the outputs\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Decision Tree (Training Model)using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the plot tree method\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# output size of decision tree\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "# providing the training dataset\n",
    "clf = clf.fit(X_train, y_train)\n",
    "plot_tree(clf, filled=True)\n",
    "plt.title(\"Decision tree training for training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Decision Tree (Testing Dataset)using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the plot tree method\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# output size of decision tree\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "# providing the training dataset\n",
    "clf = clf.fit(X_test, y_test)\n",
    "plot_tree(clf, filled=True)\n",
    "plt.title(\"Decision tree training for training dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Text based Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the tree\n",
    "from sklearn import tree\n",
    "\n",
    "# text based tree\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brazildata=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
    "df_brazildata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error for our algorithm is 68.5, which is less than 13 percent of the mean of all the values in the 'crop_value' column. This means that our algorithm did a fine prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries & Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the below 6 columns from our regression\n",
    "df_brazildata.drop('months', axis=1, inplace=True)\n",
    "df_brazildata.drop('country', axis=1, inplace=True)\n",
    "df_brazildata.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_brazildata.drop('crop_measurement', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_brazildata = pd.get_dummies(df_brazildata, columns = ['crops_action','crops_type'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_brazildata.iloc[:, 1:91].values\n",
    "y = df_brazildata.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Random Forest Regression model on the whole dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model Performance\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression on the other Target Consumer Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_brazildata = pd.read_csv('df_brazilmodels.csv')\n",
    "df_brazildata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_brazildata.drop(columns=[\"Unnamed: 0\",\"country\"],axis=1,inplace=True)\n",
    "df_brazildata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_brazildata = pd.get_dummies(df_brazildata, columns = ['crops_action','crops_type','months','crop_measurement'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_brazildata['consumer_value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_brazildata.drop('consumer_value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Multiple Linear Regression model on the Training set\n",
    "#This is the same code we used in Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "#Remember that we need to check our training results on the Test set but we can't plot a graph\n",
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2) #we display values with only 2 decimals after the comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the precision of the model or r^2\n",
    "print('The precision of the model is ')\n",
    "print(regressor.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build the equation\n",
    "print('a = ')\n",
    "print(regressor.coef_)\n",
    "\n",
    "print('The interception is: ')\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU Price of Crops in Euros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from cmath import sqrt\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "europeagri_consumer = pd.read_csv('EU/apri_ap_crpouta_1_Data.csv')\n",
    "europeagri_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europeagri_consumer.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europeagri_consumer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the datatype of Value from object to a float must be careful to check if there are no numbers in the column they\n",
    "#will be come NAN\n",
    "europeagri_consumer['Value'] = pd.to_numeric(europeagri_consumer['Value'],errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(europeagri_consumer.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europeagri_consumer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europeagri_consumer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europeagri_consumer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for and Handle MIssing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for NAN values if any\n",
    "\n",
    "europeagri_consumer.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the sum of null records\n",
    "\n",
    "europeagri_consumer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using boxplot to check for outliers in the feature Value\n",
    "sns.boxplot(x=europeagri_consumer['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking dataframe for missing data\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 5))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(europeagri_consumer.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To zoom in on the missing data\n",
    "missing_data = europeagri_consumer.iloc[:, 4:5]\n",
    "plt.figure(figsize=(20, 10))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(missing_data.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To zoom in on the column crop-value\n",
    "missing_data = europeagri_consumer.iloc[:, 4:5]\n",
    "plt.figure(figsize=(20, 10))\n",
    "colourmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(missing_data.isnull(), cmap=colourmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check rows where dependent variable is equal to zero\n",
    "europeagri_consumer.loc[europeagri_consumer['Value']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see for the value column there are over 4613 rows with the value 0 I want to find the mean value of the column\n",
    "print(europeagri_consumer['Value'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all rows with zero for the column crop_value replace them with the average value listed above 149249.4953422438\n",
    "europeagri_consumer['Value'] = europeagri_consumer['Value'].replace(0, 36.05860440713529) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check cardinality check the unique values\n",
    "\n",
    "v_cardinality = europeagri_consumer.nunique()\n",
    "\n",
    "print(v_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currency has 2 values I only want Euro so there should only be 1\n",
    "print(pd.unique(europeagri_consumer['CURRENCY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "europeagri_consumer[~europeagri_consumer.CURRENCY.str.contains(\"National currency\") == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all strings in all rows with the string 'National currency' for the feature CURRENCY\n",
    "europeagri_consumer = europeagri_consumer[~europeagri_consumer['CURRENCY'].isin(['National currency'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking now that there is only one string Euro under the feature CURRENCY\n",
    "print(pd.unique(europeagri_consumer['CURRENCY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique values of Geo\n",
    "print(pd.unique(europeagri_consumer['GEO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all strings in all rows with the string 'National United Kingdom as they are no longer in the EU' for the feature CURRENCY\n",
    "europeagri_consumer = europeagri_consumer[~europeagri_consumer['GEO'].isin(['United Kingdom'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geo now does not include the country United Kingdom\n",
    "print(pd.unique(europeagri_consumer['GEO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for NAN values if any\n",
    "\n",
    "europeagri_consumer.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give number of NaN values in every column\n",
    "europeagri_consumer.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This will give number of NaN values in every row\n",
    "europeagri_consumer.isnull().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Value column where there are NAN's with the mean of that column  If my datatset was larger I would discard\n",
    "europeagri_consumer['Value'].fillna((europeagri_consumer['Value'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give number of NaN values in every column\n",
    "europeagri_consumer.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename our columns to be more descriptive names for some and ensure all of the uppercase ones are \n",
    "#lowercase to keep in line with naming conventions\n",
    "europeagri_consumer = europeagri_consumer.rename(columns={'TIME':'year',\n",
    "                       'GEO':'country',\n",
    "                       'CURRENCY':'euro',\n",
    "                       'PROD_VEG':'produce_price_load',\n",
    "                       'Value':'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder and reindexing Columns\n",
    "\n",
    "europecolumns = [\"year\",\"country\",\"euro\",\"produce_price_load\",\"value\"]\n",
    "europeagri_consumer = europeagri_consumer.reindex(columns = europecolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change all year type to ints to ensure there are no strings for the years greater than 2009 and this covers the years for this data 2010 to 2020\n",
    "#The second 3rd line we we ensure that year is ints again sometimes after sorting or merging they can turn into strings\n",
    "\n",
    "df_europeagri_consumer = europeagri_consumer[europeagri_consumer['year'].astype('int64') > 2009]\n",
    "df_europeagri_consumer = df_europeagri_consumer.sort_values(by=['country', 'year'])\n",
    "df_europeagri_consumer['year'] = df_europeagri_consumer['year'].astype('int64')\n",
    "df_europeagri_consumer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot visualisation to depict the crop price from 2011 to 2020\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(y='value', x='year',data=df_europeagri_consumer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Were creating two variables for our min and max values for the slider\n",
    "xmin,xmax=min(df_europeagri_consumer[\"year\"]), max(df_europeagri_consumer[\"year\"])\n",
    "ymin,ymax=min(df_europeagri_consumer[\"value\"]), max(df_europeagri_consumer[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.scatter(df_europeagri_consumer, x=\"year\", y=\"value\",\n",
    "animation_frame=\"year\",\n",
    "animation_group=\"produce_price_load\", color=\"country\", hover_name=\"produce_price_load\",\n",
    "size_max = 50,\n",
    "range_x=[0.1, 50], range_y=[100,60000],\n",
    "log_x=True, log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_europeagri_consumer, x='year', y='value',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#df = px.data.gapminder().query(\"country == 'Canada'\")\n",
    "fig = px.bar(df_europeagri_consumer, x='year', y='value',\n",
    "             hover_data=['country', 'produce_price_load'], color='produce_price_load',\n",
    "             labels={'country':'Crops Produced each year by value and country'}, height=400, title='Evolution of world GDP')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "#df = px.data.gapminder()\n",
    "gdp = df_europeagri_consumer['country']\n",
    "fig = px.bar(df_europeagri_consumer, x='year', y='value', color='country', labels={'produce_price_load':'value'},\n",
    "             hover_data=['produce_price_load'],\n",
    "             title='Evolution of world crop produce')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAlue of crops produced by all Eu countries \n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.bar(df_europeagri_consumer, x='country', y='value',\n",
    "             hover_data=['country', 'produce_price_load'], color='produce_price_load',\n",
    "             labels={'country':'Crops Produced each year by value and country'}, height=400, title='Value of crops produced by each EU Country')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The value of barley for 4 EU countries for all years 2011 to 2020\n",
    "import plotly.express as px\n",
    "#df = px.data.tips()\n",
    "fig = px.bar(df_europeagri_consumer, x=\"year\", y=\"value\", color=\"year\", barmode=\"group\", facet_row=\"produce_price_load\", facet_col=\"year\",\n",
    "       category_orders={\"country\": [\"ireland\", \"spain\", \"france\", \"germany\"], \"produce_price_load\": [\"barley - prices per 100kg\"]})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europeagri_consumer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics for the Target feature: crop value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#inspect the dataframe for outliers will display the min,max mean and inner and outer quariles for all features\n",
    "df_europeagri_consumer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for the target feature crop_value\n",
    "plt.figure(figsize = (16,4))\n",
    "sns.boxplot(x= df_europeagri_consumer['value'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mean of the target feature\n",
    "mean_eurovalue= df_europeagri_consumer['value'].mean()\n",
    "print(mean_eurovalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the median of the target feature\n",
    "median = df_europeagri_consumer['value'].median()\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the mode of the target feature\n",
    "mode = df_europeagri_consumer['value'].mode()\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histplot to display the distribution of crap _value\n",
    "ax = sns.histplot(df_europeagri_consumer.value)\n",
    "ax.set(xlabel='Crop Price', ylabel='Cropvalue', title ='Cropvalue Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot to depict Box plot Crop Value by Crop Action\n",
    "sns.boxplot(x = 'year', y ='value', data=df_europeagri_consumer, hue = 'country')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "plt.title('Box plot Crop Value by Crop Action', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "ax = sns.barplot(x=\"year\", y=\"value\", data=df_europeagri_consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check cardinality of all columns and the unique values for each\n",
    "\n",
    "v_cardinality = df_europeagri_consumer.nunique()\n",
    "\n",
    "print(v_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot cardinality using a barplot\n",
    "\n",
    "df_europeagri_consumer.nunique().plot.bar(figsize=(12,6))\n",
    "\n",
    "plt.ylabel('Number of unique categories')\n",
    "\n",
    "plt.xlabel('Variables')\n",
    "\n",
    "plt.title('Cardinality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.regplot(x = df_europeagri_consumer[\"year\"], y = df_europeagri_consumer[\"value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correlation matrix displaying pearson correlation coefficients for all variables\n",
    "corr_matrix = df_europeagri_consumer.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot pair grid with histograms and scatterplots using seaborn pairgrid\n",
    "eu_data_sample = df_europeagri_consumer.sample(1000)\n",
    "p = sns.PairGrid(data=eu_data_sample, vars=[\"year\",\"country\",\"produce_price_load\",\"value\"])\n",
    "p.map_diag(plt.hist)\n",
    "p.map_offdiag(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coorelation Matrix to compare the relationships and correlation between features\n",
    "corr = df_europeagri_consumer.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, annot_kws={'size':15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point plot using seaborn pointplot to display Crop Value by month for each year from 2010 to 2020\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_europeagri_consumer, x='year', y='value', ci=\"sd\", hue='country', palette='gist_rainbow_r')\n",
    "ax1.set(title='Crop value by country for each year from 2010 to 2020')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point plot comparing Crop Value and the type of crop action for each year from 2010 to 2020 using seaborn pointplot\n",
    "fig,(ax1)= plt.subplots(nrows=1)\n",
    "fig.set_size_inches(18,5)\n",
    "sns.pointplot(data= df_europeagri_consumer, x='year', y='value', ci=\"sd\", hue='produce_price_load', palette='YlGnBu')\n",
    "#place legend outside top right corner of plot\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "ax1.set(title='Crop Value and the type of crop action for each year from 2010 to 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_europeagri_consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distribution and Null Hypothesis Tests / Inferential Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create Bland-Altman plot                  \n",
    "f, ax = plt.subplots(1, figsize = (8,5))\n",
    "sm.graphics.mean_diff_plot(df_irishdata.'Barley - prices per 100 kg', df_europeagri_consumer.'Barley - prices per 100 kg', ax = ax)\n",
    "\n",
    "#display Bland-Altman plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the data has been explored and cleaned saving it to be used for the models\n",
    "df_europeagri_consumer.to_csv('df_eumodels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage to this is I dont have to run the code from the top of the page to the bottom every time, I just need to run the imports beforhand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU Model 1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Irish Crops dataset from between 2010 to 2020 8 actions 10 crop measurements and 139 crop types\n",
    "df_eudata = pd.read_csv('df_eumodels.csv')\n",
    "df_eudata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_eudata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking Correlation using heatmap\n",
    "sns.heatmap(df_eudata.corr(),annot=True,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "-1 indicates a perfectly negative linear correlation between two variables. 0 indicates no linear correlation between two variables. 1 indicates a perfectly positive linear correlation between two variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.catplot(x='produce_price_load', data=df_eudata, palette=\"hls\",kind='count',hue='year')\n",
    "plt.xlabel(\"produce_price_load\", fontsize=16)\n",
    "plt.ylabel(\"Count\", fontsize=16)\n",
    "plt.title(\"Crops_action Grouped Count by yield, harvest and production\", fontsize=20)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataframe\n",
    "df_eudata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping ID and Source column from the data for outlier analysis\n",
    "df_eudata.drop(columns=[\"Unnamed: 0\",\"euro\"],axis=1,inplace=True)\n",
    "df_eudata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several categorical variables and will need to transform them into dummy variables with binary values in order to incorporate them into our model. This is done because despite having int values, they are not ordinal variables. In order to avoid multicollinearity we will also have to drop one of the dummy variables from each set. For example, 'month','produce_price_load','country' the first variable will be dropped for our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary dummy variables from categorical variables and drop first column to avoid multicollinearity\n",
    "df_eudata = pd.get_dummies(df_eudata, columns = ['produce_price_load','country'],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect bike_data df with added dummy variables\n",
    "df_eudata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns for Nans\n",
    "df_eudata.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = df_eudata['value']\n",
    "\n",
    "# set the independent predictor variables\n",
    "X = df_eudata.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Training split input- \", X_train.shape)\n",
    "print(\"Testing split input- \", X_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Multiple Linear Regression model on the Training set\n",
    "#This is the same code we used in Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "#Remember that we need to check our training results on the Test set but we can't plot a graph\n",
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2) #we display values with only 2 decimals after the comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the precision of the model or r^2\n",
    "print('The precision of the model is ')\n",
    "print(regressor.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build the equation\n",
    "print('a = ')\n",
    "print(regressor.coef_)\n",
    "\n",
    "print('The interception is: ')\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### LINEAR REGRESSION CURVE\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.regplot(y_test,y_pred,df_eudata,color='purple')\n",
    "plt.xlabel(\"Y TEST\")\n",
    "plt.ylabel(\"Y_PRED\")\n",
    "plt.title(\"LINEAR REGRESSION CURVE\")\n",
    "plt.savefig(\"lr3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# create Adjusted R2 Score\n",
    "p = len(X_test_1.columns)\n",
    "n = y_test.shape[0]\n",
    "adj_r2 = 1 - (1 - r2) * ((n - 1)/(n-p-1))\n",
    "\n",
    "# create RMSE score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "\n",
    "\n",
    "# print final model performance stats\n",
    "print(str(p) + \" Predictors in Test Set\")\n",
    "print(str(n) + \" Records in Test Set\")\n",
    "print(\"R2: \" + str(r2))\n",
    "print(\"Adj R2: \" + str(adj_r2))\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the R2_Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 EU Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.4f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.4f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 10\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 0.1\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis farming Irish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install autocorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imports\n",
    "import string # from some string manipulation tasks\n",
    "import nltk # natural language toolkit\n",
    "import re # regex\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "from string import punctuation # solving punctuation problems\n",
    "from nltk.corpus import stopwords # stop words in sentences\n",
    "from nltk.stem import WordNetLemmatizer # For stemming the sentence\n",
    "from nltk.stem import SnowballStemmer # For stemming the sentence\n",
    "from contractions import contractions_dict # to solve contractions\n",
    "from autocorrect import Speller #correcting the spellings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = pd.read_csv('agriireland.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan or missing values from multiple colu\n",
    "train =  train.dropna()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tweet_text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a feature word count that count s the word count in each string.  EAch tweet will be a sting. What split does\n",
    "#is look for individual word structures.\n",
    "train['word_count'] = train['tweet_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['tweet_text','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['char_count'] = train['tweet_text'].str.len() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this also includes spaces\n",
    "train[['tweet_text','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['avg_word'] = train['tweet_text'].apply(lambda x: avg_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['tweet_text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going to take each one of the tweets x represents them, its going to go through each one of the words\n",
    "#x.split then its going to check each individual word to see if its in the stoplist if the word is we keep it\n",
    "#then we get the length and return it inot the stopwords new feature.\n",
    "train['stopwords'] = train['tweet_text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "train[['tweet_text','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hastags'] = train['tweet_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "train[['tweet_text','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['numerics'] = train['tweet_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "train[['tweet_text','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['upper'] = train['tweet_text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "train[['tweet_text','upper']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_text'] = train['tweet_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To clean all of the special characters out o fthe text use below an replaced by what is after comma look up regex for all of these.\n",
    "train['tweet_text'] = train['tweet_text'].str.replace('[^\\w\\s]','')\n",
    "train['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train['tweet_text'] = train['tweet_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(train['tweet_text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet_text'] = train['tweet_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(train['tweet_text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "train['tweet_text'] = train['tweet_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['tweet_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "train['tweet_text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "TextBlob(train['tweet_text'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "train['tweet_text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(train['tweet_text'][1]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (train['tweet_text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet_text'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet_text'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet_text'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the first 5 for sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results displays the polarity is it negative positive on the right it shows us It is negative and on the right there is a definite subjectivity if there was a 1 interested in the one on the left 0 is neutral 1 is positive and -1 negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_text'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 0 in senriment because this time we want to capture the sentiment only.\n",
    "train['sentiment'] = train['tweet_text'].apply(lambda x: TextBlob(x).sentiment[1] )\n",
    "train[['tweet_text','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  #linear algebra\n",
    "import pandas as pd                 # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt     #For Visualisation\n",
    "%matplotlib inline\n",
    "import seaborn as sns               #For better Visualisation\n",
    "from bs4 import BeautifulSoup       #For Text Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the data has been explored and cleaned saving it to be used for the model\n",
    "train.to_csv('df_agrisentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in df_irishdmodels cleaned dataset\n",
    "agrisentiment = pd.read_csv('df_agrisentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment['sentiment'] = agrisentiment['sentiment'].apply(np.int64)\n",
    "print(agrisentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment = agrisentiment.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrisentiment.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count=agrisentiment.groupby('sentiment').count()\n",
    "plt.bar(sentiment_count.index.values, sentiment_count['tweet_text'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(agrisentiment['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, agrisentiment['sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(agrisentiment['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, agrisentiment['sentiment'], test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
